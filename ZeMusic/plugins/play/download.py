# -*- coding: utf-8 -*-
"""
ğŸš€ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø®Ø§Ø±Ù‚ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© V2
=====================================================
ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ù„ÙŠØ¯Ø¹Ù…:
- 5000 Ù…Ø¬Ù…ÙˆØ¹Ø© Ùˆ70,000 Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø®Ø§Øµ
- ØªØ­Ù…ÙŠÙ„ Ù…ØªÙˆØ§Ø²ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©
- Ø¥Ø¯Ø§Ø±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ù…ÙˆØ§Ø±Ø¯
- Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†Ø©
- Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØªØªØ¨Ø¹ Ù…ØªÙ‚Ø¯Ù…
"""

import os
import re
import asyncio
import logging
import time
import sqlite3
import hashlib
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import Dict, Optional, List, Tuple
from itertools import cycle
from collections import defaultdict, deque
from asyncio import Semaphore
import threading
import aiohttp
import aiofiles
from telethon.tl.types import DocumentAttributeAudio
from pathlib import Path
import uvloop
import psutil
import random
import string
import atexit
from contextlib import asynccontextmanager
import orjson

# ØªØ·Ø¨ÙŠÙ‚ UVLoop Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ asyncio

def get_audio_duration(file_path: str) -> int:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¯Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ"""
    try:
        if not os.path.exists(file_path):
            return 0
            
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        if yt_dlp:
            try:
                with yt_dlp.YoutubeDL({'quiet': True}) as ydl:
                    info = ydl.extract_info(file_path, download=False)
                    duration = info.get('duration', 0)
                    if duration and duration > 0:
                        return int(duration)
            except:
                pass
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… ffprobe
        try:
            import subprocess
            result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-show_entries', 
                'format=duration', '-of', 'csv=p=0', file_path
            ], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                return int(float(result.stdout.strip()))
        except:
            pass
            
        # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù (Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©)
        try:
            file_size = os.path.getsize(file_path)
            # ØªÙ‚Ø¯ÙŠØ±: 128kbps = 16KB/s ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
            estimated_duration = file_size // 16000
            return max(1, estimated_duration)
        except:
            return 0
            
    except Exception as e:
        LOGGER.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¯Ø© Ø§Ù„ØµÙˆØª: {e}")
        return 0
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
try:
    import yt_dlp
except ImportError:
    yt_dlp = None
    
try:
    from youtube_search import YoutubeSearch
    YOUTUBE_SEARCH_AVAILABLE = True
except ImportError:
    YoutubeSearch = None
    YOUTUBE_SEARCH_AVAILABLE = False

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Telethon
from telethon import events
from telethon.types import Message

import config
from ZeMusic.core.telethon_client import telethon_manager
from ZeMusic.logging import LOGGER
from ZeMusic.utils.database import is_search_enabled, is_search_enabled1
# from ZeMusic.utils.monitoring import PerformanceMonitor

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ ---
REQUEST_TIMEOUT = 8
DOWNLOAD_TIMEOUT = 90
MAX_SESSIONS = min(100, (psutil.cpu_count() * 4))  # Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
MAX_WORKERS = min(200, (psutil.cpu_count() * 10))  # Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬

# Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ (ÙŠÙˆØ²Ø± Ø£Ùˆ ID)
SMART_CACHE_CHANNEL = config.CACHE_CHANNEL_ID
DATABASE_PATH = "zemusic.db"
DB_FILE = DATABASE_PATH  # ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

def normalize_arabic_text(text: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù†"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
    import re
    text = re.sub(r'[\u064B-\u065F\u0670\u0640]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)  # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙ‚Ø·
    text = re.sub(r'\s+', ' ', text).strip()  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    return text

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¶
channel = getattr(config, 'STORE_LINK', '')
lnk = f"https://t.me/{channel}" if channel else None

# --- ØªØ¯ÙˆÙŠØ± Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ø®ÙˆØ§Ø¯Ù… ---
YT_API_KEYS = config.YT_API_KEYS
API_KEYS_CYCLE = cycle(YT_API_KEYS) if YT_API_KEYS else None

INVIDIOUS_SERVERS = config.INVIDIOUS_SERVERS
INVIDIOUS_CYCLE = cycle(INVIDIOUS_SERVERS) if INVIDIOUS_SERVERS else None

# ØªØ¯ÙˆÙŠØ± Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²
COOKIES_FILES = config.COOKIES_FILES
COOKIES_CYCLE = cycle(COOKIES_FILES) if COOKIES_FILES else None

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª yt-dlp Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¯Ø§Ø¡ ---
def get_ytdlp_opts(cookies_file=None) -> Dict:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù€ yt-dlp Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    opts = {
        "format": "bestaudio/best",
        "noplaylist": True,
        "quiet": True,
        "retries": 2,
        "no-cache-dir": True,
        "ignoreerrors": True,
        "socket-timeout": REQUEST_TIMEOUT,
        "force-ipv4": True,
        "throttled-rate": "1M",
        "extractor-args": "youtube:player_client=android,web",
        "concurrent-fragments": 16,  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        "outtmpl": "downloads/%(id)s.%(ext)s",
        "postprocessors": [{
            "key": "FFmpegExtractAudio",
            "preferredcodec": "mp3",
            "preferredquality": "192",
        }],
        "postprocessor_args": ["-ar", "44100", "-threads", "4"],
        "noprogress": True,
        "verbose": False,
        "http-chunk-size": "1M",
        "limit-rate": "5M",
        "buffer-size": "16M",
        "extractor-retries": 3,
        "fragment-retries": 5,
        "skip-unavailable-fragments": True,
        "merge-output-format": "mp3",
    }
    
    if cookies_file and os.path.exists(cookies_file):
        opts["cookiefile"] = cookies_file
    
    # Ø¥Ø¶Ø§ÙØ© aria2c Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±
    import shutil
    if shutil.which("aria2c"):
        opts.update({
            "external_downloader": "aria2c",
            "external_downloader_args": [
                "-x", "16", 
                "-s", "16", 
                "-k", "2M",
                "--file-allocation=none",
                "--summary-interval=0"
            ],
        })
    
    return opts

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª
os.makedirs("downloads", exist_ok=True)

# --- Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø°ÙƒÙŠØ© ---
# DB_FILE ØªÙ… ØªØ¹Ø±ÙŠÙÙ‡ ÙÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰

async def init_database():
    """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    
    # ØªØ­Ø³ÙŠÙ† Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙˆÙ„
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS channel_index (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            message_id INTEGER UNIQUE,
            file_id TEXT UNIQUE,
            file_unique_id TEXT,
            
            search_hash TEXT UNIQUE,
            title_normalized TEXT,
            artist_normalized TEXT,
            keywords_vector TEXT,
            
            original_title TEXT,
            original_artist TEXT,
            duration INTEGER,
            file_size INTEGER,
            
            access_count INTEGER DEFAULT 0,
            last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            popularity_rank REAL DEFAULT 0,
            
            phonetic_hash TEXT,
            partial_matches TEXT,
            
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ‡Ø§Ø±Ø³
    indexes = [
        "CREATE INDEX IF NOT EXISTS idx_search_hash ON channel_index(search_hash)",
        "CREATE INDEX IF NOT EXISTS idx_title_norm ON channel_index(title_normalized)",
        "CREATE INDEX IF NOT EXISTS idx_artist_norm ON channel_index(artist_normalized)",
        "CREATE INDEX IF NOT EXISTS idx_popularity ON channel_index(popularity_rank DESC)",
        "CREATE INDEX IF NOT EXISTS idx_message_id ON channel_index(message_id)",
        "CREATE INDEX IF NOT EXISTS idx_file_id ON channel_index(file_id)",
        "CREATE INDEX IF NOT EXISTS idx_keywords ON channel_index(keywords_vector)"
    ]
    
    for index_sql in indexes:
        cursor.execute(index_sql)
    
    conn.commit()
    conn.close()

# ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ÙˆØ­Ø¯Ø©
# Ø³ÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ Ø£ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù…
_database_initialized = False

async def ensure_database_initialized():
    """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    global _database_initialized
    if not _database_initialized:
        await init_database()
        _database_initialized = True

# ================================================================
#                 Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# ================================================================
class ConnectionManager:
    """Ù…Ø¯ÙŠØ± Ø§ØªØµØ§Ù„Ø§Øª Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ù„Ø³Ø§Øª"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._session_pool = []
            cls._instance._executor_pool = None
            cls._instance._db_connections = {}
        return cls._instance
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© ØªØ¬Ù…Ø¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        # ØªØ¬Ù…Ø¹ Ø¬Ù„Ø³Ø§Øª HTTP
        connector = aiohttp.TCPConnector(
            limit_per_host=200,
            ttl_dns_cache=600,
            use_dns_cache=True,
            keepalive_timeout=45,
            enable_cleanup_closed=True
        )
        
        self._session_pool = [
            aiohttp.ClientSession(
                connector=connector,
                timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT),
                headers={'User-Agent': f'ZeMusic-{i}'},
                json_serialize=orjson.dumps
            ) for i in range(MAX_SESSIONS)
        ]
        
        # ØªØ¬Ù…Ø¹ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°
        self._executor_pool = concurrent.futures.ThreadPoolExecutor(
            max_workers=MAX_WORKERS,
            thread_name_prefix="DLWorker"
        )
        
        LOGGER(__name__).info(f"ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª: {MAX_SESSIONS} Ø¬Ù„Ø³Ø©, {MAX_WORKERS} Ø¹Ø§Ù…Ù„")
    
    @property
    def session_pool(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¬Ù…Ø¹ Ø§Ù„Ø¬Ù„Ø³Ø§Øª"""
        if not self._session_pool:
            raise RuntimeError("ConnectionManager not initialized")
        return self._session_pool
    
    @property
    def executor_pool(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¬Ù…Ø¹ Ø§Ù„Ø¹Ù…Ø§Ù„"""
        if not self._executor_pool:
            raise RuntimeError("Executor pool not initialized")
        return self._executor_pool
    
    async def get_session(self) -> aiohttp.ClientSession:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù„Ø³Ø© Ù…ØªØ§Ø­Ø©"""
        if not self._session_pool:
            await self.initialize()
        return random.choice(self._session_pool)
    
    @asynccontextmanager
    async def db_connection(self):
        """Ø¥Ø¯Ø§Ø±Ø© Ø§ØªØµØ§Ù„Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(DB_FILE, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()
    
    async def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            if self._session_pool:
                for session in self._session_pool:
                    if session and not session.closed:
                        await session.close()
            if self._executor_pool:
                self._executor_pool.shutdown(wait=True)
            LOGGER(__name__).info("ğŸ”Œ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„")
        except Exception as e:
            LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯: {e}")

# ================================================================
#                 Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø®Ø§Ø±Ù‚
# ================================================================
class HyperSpeedDownloader:
    """Ù†Ø³Ø®Ø© Ù…Ø¨Ø³Ø·Ø© Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
    
    def __init__(self):
        self.downloads_folder = "downloads"
        os.makedirs(self.downloads_folder, exist_ok=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        self.cache_hits = 0
        self.cache_misses = 0
        self.active_tasks = set()
        self.last_health_check = time.time()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.method_performance = {
            'youtube_api': {'avg_time': 0},
            'invidious': {'avg_time': 0},
            'youtube_search': {'avg_time': 0},
            'ytdlp_cookies': {'avg_time': 0},
            'ytdlp_no_cookies': {'avg_time': 0}
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¯ÙŠØ± Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª
        try:
            self.conn_manager = ConnectionManager()
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª: {e}")
            self.conn_manager = None
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
        LOGGER(__name__).info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†")
    
    async def search_in_smart_cache(self, query: str) -> Optional[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ø¢Ù„ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹
            normalized_query = normalize_arabic_text(query)
            
            conn = sqlite3.connect(DATABASE_PATH)
            cursor = conn.cursor()
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„ÙÙ†Ø§Ù†
            cursor.execute("""
                SELECT video_id, title, artist, duration, file_path, thumb, message_id, keywords
                FROM cached_audio 
                WHERE LOWER(title) LIKE ? OR LOWER(artist) LIKE ? OR LOWER(keywords) LIKE ?
                ORDER BY created_at DESC LIMIT 5
            """, (f'%{normalized_query.lower()}%', f'%{normalized_query.lower()}%', f'%{normalized_query.lower()}%'))
            
            results = cursor.fetchall()
            conn.close()
            
            if results:
                result = results[0]  # Ø£Ø®Ø° Ø£ÙˆÙ„ Ù†ØªÙŠØ¬Ø©
                self.cache_hits += 1
                return {
                    "video_id": result[0],
                    "title": result[1],
                    "artist": result[2],
                    "duration": result[3],
                    "file_path": result[4],
                    "thumb": result[5],
                    "message_id": result[6],
                    "source": "smart_cache"
                }
            
            self.cache_misses += 1
            return None
            
        except Exception as e:
            LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹: {e}")
            self.cache_misses += 1
            return None
    
    async def health_check(self):
        """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ"""
        if time.time() - self.last_health_check > 300:  # ÙƒÙ„ 5 Ø¯Ù‚Ø§Ø¦Ù‚
            self.last_health_check = time.time()
            
            # ØªØ³Ø¬ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
            stats = {
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'cache_hit_rate': self.cache_hits / max(1, self.cache_hits + self.cache_misses) * 100,
                'active_tasks': len(self.active_tasks),
                'memory_usage': psutil.virtual_memory().percent,
                'cpu_usage': psutil.cpu_percent(),
            }
            
            LOGGER(__name__).info(
                f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…: "
                f"Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {stats['memory_usage']}% | "
                f"Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {stats['cpu_usage']}% | "
                f"Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {stats['active_tasks']} | "
                f"Ù†Ø³Ø¨Ø© Ø§Ù„ÙƒØ§Ø´: {stats['cache_hit_rate']:.1f}%"
            )
    
    def normalize_text(self, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ù„Ù„Ø¨Ø­Ø« Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if not text:
            return ""
        
        # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø£Ø­Ø±Ù Ø§Ù„ØµØºÙŠØ±Ø© ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = text.lower()
        text = re.sub(r'[\u064B-\u065F]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        text = re.sub(r'[^\w\s]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ²
        text = re.sub(r'\s+', ' ', text).strip()  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø¯ÙˆÙ„ ØªØ­ÙˆÙŠÙ„
        replacements = {
            'Ø©': 'Ù‡', 'ÙŠ': 'Ù‰', 'Ø£': 'Ø§', 'Ø¥': 'Ø§',
            'Ø¢': 'Ø§', 'Ø¤': 'Ùˆ', 'Ø¦': 'ÙŠ', 'Ù±': 'Ø§',
            'Ù°': '', 'Ù‘': '', 'Ù’': '', 'ÙŒ': '',
            'Ù': '', 'Ù‹': '', 'Ù': '', 'Ù': '',
            'Ù': '', '~': '', 'Ù€': ''
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text
    
    def create_search_hash(self, title: str, artist: str = "") -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø£Ø³Ø±Ø¹"""
        normalized_title = self.normalize_text(title)
        normalized_artist = self.normalize_text(artist)
        combined = f"{normalized_title}_{normalized_artist}".encode()
        return hashlib.md5(combined, usedforsecurity=False).hexdigest()[:12]
    
    async def lightning_search_cache(self, query: str) -> Optional[Dict]:
        """Ø¨Ø­Ø« Ø®Ø§Ø·Ù ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            normalized_query = self.normalize_text(query)
            search_hash = self.create_search_hash(normalized_query)
            
            async with self.conn_manager.db_connection() as conn:
                cursor = conn.cursor()
                
                # Ø¨Ø­Ø« Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ù„Ù‡Ø§Ø´
                cursor.execute(
                    "SELECT message_id, file_id, original_title, original_artist, duration "
                    "FROM channel_index WHERE search_hash = ? LIMIT 1",
                    (search_hash,)
                )
                result = cursor.fetchone()
                
                if result:
                    # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                    cursor.execute(
                        "UPDATE channel_index SET access_count = access_count + 1, "
                        "last_accessed = CURRENT_TIMESTAMP WHERE search_hash = ?",
                        (search_hash,)
                    )
                    conn.commit()
                    
                    self.cache_hits += 1
                    return {
                        'message_id': result[0],
                        'file_id': result[1],
                        'title': result[2],
                        'artist': result[3],
                        'duration': result[4],
                        'source': 'cache',
                        'cached': True
                    }
                
                # Ø¨Ø­Ø« ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ‡Ø±Ø³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
                keywords = normalized_query.split()
                keyword_conditions = " OR ".join(["keywords_vector LIKE ?" for _ in keywords])
                params = [f"%{kw}%" for kw in keywords]
                
                cursor.execute(
                    f"SELECT message_id, file_id, original_title, original_artist, duration "
                    f"FROM channel_index WHERE {keyword_conditions} "
                    f"ORDER BY popularity_rank DESC LIMIT 1",
                    params
                )
                result = cursor.fetchone()
                
                if result:
                    self.cache_hits += 1
                    return {
                        'message_id': result[0],
                        'file_id': result[1],
                        'title': result[2],
                        'artist': result[3],
                        'duration': result[4],
                        'source': 'cache_fuzzy',
                        'cached': True
                    }
            
            self.cache_misses += 1
        except Exception as e:
            LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹: {e}")
            # self.monitor.log_error('cache_search')
        
        return None
    
    async def youtube_api_search(self, query: str) -> Optional[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± YouTube Data API Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if not API_KEYS_CYCLE:
            return None
        
        session = await self.conn_manager.get_session()
        start_time = time.time()
        
        try:
            for attempt in range(len(YT_API_KEYS)):
                key = next(API_KEYS_CYCLE)
                LOGGER(__name__).info(f"ğŸ”‘ Ù…Ø­Ø§ÙˆÙ„Ø© YouTube API - Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}")
                params = {
                    "part": "snippet",
                    "q": query,
                    "type": "video",
                    "maxResults": 1,
                    "key": key,
                    "videoCategoryId": "10",  # Ù…ÙˆØ³ÙŠÙ‚Ù‰ ÙÙ‚Ø·
                    "relevanceLanguage": "ar,en"
                }
                
                try:
                    async with session.get(
                        "https://www.googleapis.com/youtube/v3/search",
                        params=params,
                        timeout=REQUEST_TIMEOUT
                    ) as resp:
                        if resp.status == 403:
                            LOGGER(__name__).warning(f"Ù…ÙØªØ§Ø­ API Ù…Ø­Ø¸ÙˆØ±: {key[:5]}...")
                            continue
                            
                        if resp.status != 200:
                            error_text = await resp.text()
                            LOGGER(__name__).warning(f"YouTube API Ø®Ø·Ø£ {resp.status}: {error_text[:100]}")
                            continue
                        
                        data = await resp.json()
                        items = data.get("items", [])
                        if not items:
                            LOGGER(__name__).warning(f"YouTube API: Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {query}")
                            continue
                        
                        item = items[0]
                        video_id = item["id"]["videoId"]
                        snippet = item["snippet"]
                        title = snippet.get("title", "")[:60]
                        
                        LOGGER(__name__).info(f"âœ… YouTube API Ù†Ø¬Ø­: {title[:30]}...")
                        
                        self.method_performance['youtube_api']['avg_time'] = (
                            self.method_performance['youtube_api']['avg_time'] * 0.7 + 
                            (time.time() - start_time) * 0.3
                        )
                        
                        return {
                            "video_id": video_id,
                            "title": title,
                            "artist": snippet.get("channelTitle", "Unknown"),
                            "thumb": snippet.get("thumbnails", {}).get("high", {}).get("url"),
                            "source": "youtube_api"
                        }
                
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    continue
        
        except Exception as e:
            LOGGER(__name__).warning(f"ÙØ´Ù„ YouTube API: {e}")
            # self.monitor.log_error('youtube_api')
        
        return None
    
    async def invidious_search(self, query: str) -> Optional[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± Invidious Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if not INVIDIOUS_CYCLE:
            return None
        
        session = await self.conn_manager.get_session()
        start_time = time.time()
        
        try:
            for _ in range(len(INVIDIOUS_SERVERS)):
                server = next(INVIDIOUS_CYCLE)
                url = f"{server}/api/v1/search"
                params = {
                    "q": query, 
                    "type": "video",
                    "sort_by": "relevance",
                    "duration": "short"
                }
                
                try:
                    async with session.get(
                        url, 
                        params=params,
                        timeout=REQUEST_TIMEOUT
                    ) as resp:
                        if resp.status != 200:
                            continue
                        
                        content_type = resp.headers.get('content-type', '')
                        if 'application/json' not in content_type:
                            continue
                        
                        data = await resp.json()
                        video = next((item for item in data if item.get("type") == "video"), None)
                        if not video:
                            continue
                        
                        self.method_performance['invidious']['avg_time'] = (
                            self.method_performance['invidious']['avg_time'] * 0.7 + 
                            (time.time() - start_time) * 0.3
                        )
                        
                        return {
                            "video_id": video.get("videoId"),
                            "title": video.get("title", "")[:60],
                            "artist": video.get("author", "Unknown"),
                            "duration": int(video.get("lengthSeconds", 0)),
                            "thumb": next(
                                (t.get("url") for t in reversed(video.get("videoThumbnails", []))),
                                None
                            ),
                            "source": "invidious"
                        }
                
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    continue
        
        except Exception as e:
            LOGGER(__name__).warning(f"ÙØ´Ù„ Invidious: {e}")
            # self.monitor.log_error('invidious')
        
        return None
    
    async def youtube_search_simple(self, query: str) -> Optional[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± youtube_search Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø©"""
        if not YoutubeSearch:
            return None
        
        start_time = time.time()
            
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¨Ø­Ø«
            if not YoutubeSearch:
                LOGGER(__name__).warning(f"YouTube Search ØºÙŠØ± Ù…ØªØ§Ø­")
                return None
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… youtube_search
            LOGGER(__name__).info(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ YouTube Search: {query}")
            search = YoutubeSearch(query, max_results=1)
            results = search.to_dict()
            
            LOGGER(__name__).info(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(results) if results else 0}")
            
            if not results:
                LOGGER(__name__).warning(f"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¨Ø­Ø«: {query}")
                return None
                
            result = results[0]
            LOGGER(__name__).info(f"ğŸ“ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: {result.get('title', 'Unknown')[:30]}...")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
            video_id = result.get('id', '')
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù…Ø¹Ø±Ù Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
            if not video_id:
                url_suffix = result.get('url_suffix', '')
                link = result.get('link', '')
                
                if url_suffix and 'watch?v=' in url_suffix:
                    video_id = url_suffix.split('watch?v=')[1].split('&')[0]
                elif link and 'watch?v=' in link:
                    video_id = link.split('watch?v=')[1].split('&')[0]
            
            LOGGER(__name__).info(f"ğŸ”— URL Suffix: {result.get('url_suffix', 'Unknown')}")
            LOGGER(__name__).info(f"ğŸ†” Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬: {video_id}")
            title = result.get('title', 'Unknown Title')
            artist = result.get('channel', 'Unknown Artist')
            duration_text = result.get('duration', '0:00')
            thumb = result.get('thumbnails', [None])[0] if result.get('thumbnails') else None
            
            LOGGER(__name__).info(f"ğŸ†” Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {video_id}")
            LOGGER(__name__).info(f"ğŸµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title[:30]}...")
            LOGGER(__name__).info(f"ğŸ¤ Ø§Ù„ÙÙ†Ø§Ù†: {artist[:20]}...")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø©
            duration = 0
            if isinstance(duration_text, str) and ':' in duration_text:
                try:
                    parts = duration_text.split(':')
                    if len(parts) == 2:
                        duration = int(parts[0]) * 60 + int(parts[1])
                    elif len(parts) == 3:
                        duration = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
                except (ValueError, IndexError):
                    duration = 0
            
            if not video_id:
                return None
            
            self.method_performance['youtube_search']['avg_time'] = (
                self.method_performance['youtube_search']['avg_time'] * 0.7 + 
                (time.time() - start_time) * 0.3
            )
            
            return {
                "video_id": video_id,
                "title": title[:60],
                "artist": artist[:40] if artist else "Unknown Artist",
                "duration": duration,
                "thumb": thumb,
                "link": f"https://youtube.com/watch?v={video_id}",
                "source": "youtube_search"
            }
            
        except Exception as e:
            LOGGER(__name__).warning(f"ÙØ´Ù„ YouTube Search: {e}")
            try:
                self.monitor.log_error('youtube_search')
            except:
                pass
            return None
    
    async def download_with_ytdlp(self, video_info: Dict) -> Optional[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø¹Ø¨Ø± yt-dlp Ù…Ø¹ ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙƒÙˆÙƒÙŠØ²"""
        if not yt_dlp:
            return None
            
        video_id = video_info.get("video_id")
        if not video_id:
            return None
        
        url = f"https://youtu.be/{video_id}"
        start_time = time.time()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ Ù…Ø¯ÙŠØ± Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ø°ÙƒÙŠ
        try:
            from ZeMusic.core.cookies_manager import cookies_manager, report_cookie_success, report_cookie_failure
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø£ÙˆÙ„Ø§Ù‹
            for attempt in range(2):  # Ù…Ø­Ø§ÙˆÙ„Ø© ÙƒÙˆÙƒÙŠØ²ÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ†
                try:
                    cookies_file = await cookies_manager.get_next_cookie()
                    if not cookies_file:
                        break
                        
                    opts = get_ytdlp_opts(cookies_file)
                    
                    loop = asyncio.get_running_loop()
                    info = await loop.run_in_executor(
                        self.conn_manager.executor_pool,
                        lambda: yt_dlp.YoutubeDL(opts).extract_info(url, download=True)
                    )
                    
                    if info:
                        audio_path = f"downloads/{video_id}.mp3"
                        if os.path.exists(audio_path):
                            # ØªÙ‚Ø±ÙŠØ± Ù†Ø¬Ø§Ø­ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø¯Ø§Ø¡
                            await report_cookie_success(cookies_file)
                            self.method_performance['ytdlp_cookies']['avg_time'] = (
                                self.method_performance['ytdlp_cookies']['avg_time'] * 0.7 + 
                                (time.time() - start_time) * 0.3
                            )
                            
                            return {
                                "audio_path": audio_path,
                                "title": info.get("title", video_info.get("title", ""))[:60],
                                "artist": info.get("uploader", video_info.get("artist", "Unknown")),
                                "duration": int(info.get("duration", 0)),
                                "file_size": os.path.getsize(audio_path),
                                "source": f"ytdlp_cookies_{Path(cookies_file).name}"
                            }
                
                except Exception as e:
                    # ØªÙ‚Ø±ÙŠØ± ÙØ´Ù„
                    if 'cookies_file' in locals() and cookies_file:
                        await report_cookie_failure(cookies_file, str(e))
                    LOGGER(__name__).warning(f"ÙØ´Ù„ yt-dlp Ù…Ø¹ ÙƒÙˆÙƒÙŠØ² {cookies_file}: {e}")
                    continue
                    
        except ImportError:
            # Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù…Ø¯ÙŠØ± Ù…ØªØ§Ø­Ø§Ù‹
            if COOKIES_CYCLE:
                for _ in range(len(COOKIES_FILES)):
                    try:
                        cookies_file = next(COOKIES_CYCLE)
                        opts = get_ytdlp_opts(cookies_file)
                        
                        loop = asyncio.get_running_loop()
                        info = await loop.run_in_executor(
                            self.conn_manager.executor_pool,
                            lambda: yt_dlp.YoutubeDL(opts).extract_info(url, download=True)
                        )
                        
                        if info:
                            audio_path = f"downloads/{video_id}.mp3"
                            if os.path.exists(audio_path):
                                return {
                                    "audio_path": audio_path,
                                    "title": info.get("title", video_info.get("title", ""))[:60],
                                    "artist": info.get("uploader", video_info.get("artist", "Unknown")),
                                    "duration": int(info.get("duration", 0)),
                                    "file_size": os.path.getsize(audio_path),
                                    "source": f"ytdlp_cookies_{cookies_file}"
                                }
                    
                    except Exception as e:
                        LOGGER(__name__).warning(f"ÙØ´Ù„ yt-dlp Ù…Ø¹ ÙƒÙˆÙƒÙŠØ² {cookies_file}: {e}")
                        continue
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙˆÙ† ÙƒÙˆÙƒÙŠØ²
        try:
            opts = get_ytdlp_opts()
            loop = asyncio.get_running_loop()
            info = await loop.run_in_executor(
                self.conn_manager.executor_pool,
                lambda: yt_dlp.YoutubeDL(opts).extract_info(url, download=True)
            )
            
            if info:
                audio_path = f"downloads/{video_id}.mp3"
                if os.path.exists(audio_path):
                    self.method_performance['ytdlp_no_cookies']['avg_time'] = (
                        self.method_performance['ytdlp_no_cookies']['avg_time'] * 0.7 + 
                        (time.time() - start_time) * 0.3
                    )
                    return {
                        "audio_path": audio_path,
                        "title": info.get("title", video_info.get("title", ""))[:60],
                        "artist": info.get("uploader", video_info.get("artist", "Unknown")),
                        "duration": int(info.get("duration", 0)),
                        "file_size": os.path.getsize(audio_path),
                        "source": "ytdlp_no_cookies"
                    }
        
        except Exception as e:
            LOGGER(__name__).error(f"ÙØ´Ù„ yt-dlp Ø¨Ø¯ÙˆÙ† ÙƒÙˆÙƒÙŠØ²: {e}")
            self.monitor.log_error('ytdlp_download')
        
        return None
    
    async def cache_to_channel(self, audio_info: Dict, search_query: str) -> Optional[str]:
        """Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Telethon"""
        if not SMART_CACHE_CHANNEL or not telethon_manager.bot_client:
            return None
        
        try:
            audio_path = audio_info["audio_path"]
            title = audio_info["title"]
            artist = audio_info["artist"]
            duration = audio_info["duration"]
            file_size = audio_info["file_size"]
            
            # Ø¥Ù†Ø´Ø§Ø¡ caption Ù„Ù„Ù…Ù„Ù
            caption = f"""ğŸµ {title}
ğŸ¤ {artist}
â±ï¸ {duration}s | ğŸ“Š {file_size/1024/1024:.1f}MB
ğŸ”— {audio_info["source"]}
ğŸ” {search_query[:50]}"""
            
            # Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ù‚Ù†Ø§Ø©
            message = await telethon_manager.bot_client.send_file(
                entity=SMART_CACHE_CHANNEL,
                file=audio_path,
                caption=caption,
                attributes=[
                    DocumentAttributeAudio(
                        duration=duration,
                        title=title[:60],
                        performer=artist[:40]
                    )
                ],
                supports_streaming=True
            )
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            search_hash = self.create_search_hash(title, artist)
            normalized_title = self.normalize_text(title)
            normalized_artist = self.normalize_text(artist)
            keywords = f"{normalized_title} {normalized_artist} {self.normalize_text(search_query)}"
            
            async with self.conn_manager.db_connection() as conn:
                cursor = conn.cursor()
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ file_id Ù…Ù† Telethon
                file_id = message.document.id if message.document else None
                file_unique_id = getattr(message.document, 'access_hash', None)
                
                cursor.execute('''
                    INSERT OR REPLACE INTO channel_index 
                    (message_id, file_id, file_unique_id, search_hash, title_normalized, artist_normalized, 
                     keywords_vector, original_title, original_artist, duration, file_size)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    message.id, str(file_id), str(file_unique_id),
                    search_hash, normalized_title, normalized_artist, keywords,
                    title, artist, duration, file_size
                ))
                
                conn.commit()
            
            LOGGER(__name__).info(f"âœ… ØªÙ… Ø­ÙØ¸ {title} ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ")
            return str(file_id)
            
        except Exception as e:
            LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")
            self.monitor.log_error('cache_save')
        
        return None
    
    async def hyper_download(self, query: str) -> Optional[Dict]:
        """Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø§Ø±Ù‚ Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚"""
        task_id = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
        self.active_tasks.add(task_id)
        start_time = time.time()
        
        try:
            # ÙØ­Øµ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠ
            await self.health_check()
            
            # Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ÙÙˆØ±ÙŠ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
            cached_result = await self.search_in_smart_cache(query)
            if cached_result:
                LOGGER(__name__).info(f"âš¡ ÙƒØ§Ø´ ÙÙˆØ±ÙŠ: {query} ({time.time() - start_time:.3f}s)")
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                return {
                    'audio_path': cached_result.get('file_path'),
                    'title': cached_result.get('title', 'Unknown'),
                    'artist': cached_result.get('artist', 'Unknown'),
                    'duration': cached_result.get('duration', 0),
                    'source': 'smart_cache',
                    'cached': True
                }
            
            # Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            search_methods = []
            
            # Ø¥Ø¶Ø§ÙØ© Ø·Ø±Ù‚ Ø§Ù„Ø¨Ø­Ø« Ø¨ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
            
            # Ø£ÙˆÙ„ÙˆÙŠØ© 1: YouTube Search (Ø§Ù„Ø£ÙƒØ«Ø± Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©)
            try:
                if YoutubeSearch:
                    search_methods.append(self.youtube_search_simple(query))
                    LOGGER(__name__).info(f"ğŸ” Ø¥Ø¶Ø§ÙØ© YouTube Search Ù„Ù„Ø¨Ø­Ø«")
            except Exception as e:
                LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø¥Ø¶Ø§ÙØ© YouTube Search: {e}")
            
            # Ø£ÙˆÙ„ÙˆÙŠØ© 2: YouTube API (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
            if API_KEYS_CYCLE:
                search_methods.append(self.youtube_api_search(query))
                LOGGER(__name__).info(f"ğŸ” Ø¥Ø¶Ø§ÙØ© YouTube API Ù„Ù„Ø¨Ø­Ø«")
            
            # Ø£ÙˆÙ„ÙˆÙŠØ© 3: Invidious (ÙƒØ¨Ø¯ÙŠÙ„)
            if INVIDIOUS_CYCLE:
                search_methods.append(self.invidious_search(query))
                LOGGER(__name__).info(f"ğŸ” Ø¥Ø¶Ø§ÙØ© Invidious Ù„Ù„Ø¨Ø­Ø«")
            
            if not search_methods:
                LOGGER(__name__).error(f"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø·Ø±Ù‚ Ø¨Ø­Ø« Ù…ØªØ§Ø­Ø©!")
                return None
            
            LOGGER(__name__).info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù€ {len(search_methods)} Ø·Ø±ÙŠÙ‚Ø©")
            
            # ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            search_tasks = [asyncio.create_task(method) for method in search_methods]
            done, pending = await asyncio.wait(
                search_tasks,
                timeout=REQUEST_TIMEOUT * 1.5,
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
            for task in pending:
                task.cancel()
            
            # Ø£Ø®Ø° Ø£ÙˆÙ„ Ù†ØªÙŠØ¬Ø© Ù†Ø§Ø¬Ø­Ø©
            video_info = None
            for task in done:
                try:
                    result = task.result()
                    if result:
                        video_info = result
                        LOGGER(__name__).info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø¨Ø­Ø«: {result.get('title', 'Unknown')} Ù…Ù† {result.get('source', 'Unknown')}")
                        break
                except Exception as e:
                    LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„Øª Ø¥Ø­Ø¯Ù‰ Ø·Ø±Ù‚ Ø§Ù„Ø¨Ø­Ø«: {e}")
            
            if not video_info:
                LOGGER(__name__).error(f"âŒ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„Ø¨Ø­Ø« Ù„Ù€: {query}")
                return None
            
            # Ø®Ø·ÙˆØ© 3: ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª
            LOGGER(__name__).info(f"ğŸµ Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª: {video_info.get('title', 'Unknown')}")
            audio_info = await self.download_with_ytdlp(video_info)
            if not audio_info:
                LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©...")
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
                audio_info = await self.download_without_cookies(video_info)
                if not audio_info:
                    LOGGER(__name__).error(f"âŒ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„Ù€: {video_info.get('title', 'Unknown')}")
                    return None
                else:
                    LOGGER(__name__).info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {audio_info.get('title', 'Unknown')}")
            else:
                LOGGER(__name__).info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {audio_info.get('title', 'Unknown')}")
            
            # Ø®Ø·ÙˆØ© 4: Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ (ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©)
            if SMART_CACHE_CHANNEL:
                try:
                    # Ø³ÙŠØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø¯Ø§Ù„Ø© send_audio_file
                    pass
                except Exception as cache_error:
                    LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ†: {cache_error}")
            
            LOGGER(__name__).info(f"âœ… ØªØ­Ù…ÙŠÙ„ Ø¬Ø¯ÙŠØ¯: {query} ({time.time() - start_time:.3f}s)")
            
            return {
                'audio_path': audio_info['audio_path'],
                'title': audio_info['title'],
                'artist': audio_info['artist'],
                'duration': audio_info['duration'],
                'source': audio_info['source'],
                'cached': False
            }
            
        except Exception as e:
            LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø§Ø±Ù‚: {e}")
            # self.monitor.log_error('hyper_download')
            return None
        finally:
            self.active_tasks.discard(task_id)
    
    async def direct_ytdlp_download(self, video_id: str, title: str = "Unknown") -> Optional[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ù…Ø¨Ø³Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp"""
        if not yt_dlp:
            LOGGER(__name__).error("yt-dlp ØºÙŠØ± Ù…ØªØ§Ø­")
            return None
            
        start_time = time.time()
        LOGGER(__name__).info(f"ğŸ”„ Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„: {video_id}")
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„
            temp_dir = Path(self.downloads_folder)
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ø¹ Ø§Ù„ØªØ¯ÙˆÙŠØ± Ø§Ù„Ø°ÙƒÙŠ
            cookies_files = get_available_cookies()
            LOGGER(__name__).info(f"ğŸª Ù…ØªØ§Ø­: {len(cookies_files)} Ù…Ù„Ù ÙƒÙˆÙƒÙŠØ² Ù„Ù„ØªØ¯ÙˆÙŠØ±")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            ydl_configs = []
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…Ø¹ ÙƒÙ„ Ù…Ù„Ù ÙƒÙˆÙƒÙŠØ² Ù…Ø¹ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            distribution = calculate_cookies_distribution(len(cookies_files))
            primary_count = distribution['primary']
            
            LOGGER(__name__).info(f"ğŸª Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ: Ø£Ø³Ø§Ø³ÙŠ={primary_count}, Ø«Ø§Ù†ÙˆÙŠ={distribution['secondary']}, Ù…ØªØ¨Ù‚ÙŠ={distribution['remaining']}")
            
            for i, cookie_file in enumerate(cookies_files[:primary_count], 1):
                ydl_configs.append({
                    'format': 'bestaudio/best',
                    'outtmpl': str(temp_dir / f'{video_id}_cookie_{i}.%(ext)s'),
                    'quiet': True,
                    'no_warnings': True,
                    'noplaylist': True,
                    'socket_timeout': 15,
                    'retries': 1,
                    'cookiefile': cookie_file,
                    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    '_cookie_file': cookie_file,  # ØªØªØ¨Ø¹ Ù…Ù„Ù Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                    '_cookie_index': i  # Ø±Ù‚Ù… Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                })
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¨Ø¯ÙˆÙ† ÙƒÙˆÙƒÙŠØ² Ù…Ø¹ user agents Ù…Ø®ØªÙ„ÙØ©
            ydl_configs.extend([
                {
                    'format': 'worstaudio[ext=webm]/worstaudio[ext=m4a]/worstaudio',
                    'outtmpl': str(temp_dir / f'{video_id}_low.%(ext)s'),
                    'quiet': True,
                    'no_warnings': True,
                    'noplaylist': True,
                    'socket_timeout': 10,
                    'retries': 1,
                    'user_agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15',
                    'referer': 'https://m.youtube.com/',
                },
                {
                    'format': 'bestaudio[filesize<50M]/best[filesize<50M]',
                    'outtmpl': str(temp_dir / f'{video_id}_med.%(ext)s'),
                    'quiet': True,
                    'no_warnings': True,
                    'noplaylist': True,
                    'socket_timeout': 15,
                    'retries': 1,
                    'user_agent': 'Mozilla/5.0 (Android 10; Mobile; rv:91.0) Gecko/91.0 Firefox/91.0',
                }
            ])
            
            # Ø¬Ø±Ø¨ ÙƒÙ„ Ø¥Ø¹Ø¯Ø§Ø¯ Ø­ØªÙ‰ ÙŠÙ†Ø¬Ø­ Ø£Ø­Ø¯Ù‡Ù… Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
            for i, ydl_opts in enumerate(ydl_configs, 1):
                cookie_file = ydl_opts.get('_cookie_file')
                
                try:
                    LOGGER(__name__).info(f"ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ #{i}")
                    if cookie_file:
                        LOGGER(__name__).info(f"ğŸª Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙˆÙƒÙŠØ²: {os.path.basename(cookie_file)}")
                    
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        info = ydl.extract_info(
                            f"https://www.youtube.com/watch?v={video_id}",
                            download=True
                        )
                        
                        if info:
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„
                            LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© #{i}: {info.get('title', title)}")
                            
                            # ØªØªØ¨Ø¹ Ù†Ø¬Ø§Ø­ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                            if cookie_file:
                                track_cookie_usage(cookie_file, success=True)
                            
                            for file_path in temp_dir.glob(f"{video_id}*.*"):
                                if file_path.suffix in ['.m4a', '.mp3', '.webm', '.mp4', '.opus']:
                                    LOGGER(__name__).info(f"ğŸ“ Ù…Ù„Ù Ù…Ø­Ù…Ù„: {file_path}")
                                    return {
                                        'success': True,
                                        'file_path': str(file_path),
                                        'title': info.get('title', title),
                                        'duration': info.get('duration', 0),
                                        'uploader': info.get('uploader', 'Unknown'),
                                        'elapsed': time.time() - start_time
                                    }
                            break
                            
                except Exception as e:
                    error_msg = str(e).lower()
                    LOGGER(__name__).warning(f"âŒ ÙØ´Ù„Øª Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© #{i}: {e}")
                    
                    # ØªØªØ¨Ø¹ ÙØ´Ù„ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                    if cookie_file:
                        track_cookie_usage(cookie_file, success=False)
                    
                    # ÙØ­Øµ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø­Ø¸Ø± ÙˆØ§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                    if cookie_file and any(keyword in error_msg for keyword in [
                        'blocked', 'forbidden', '403', 'unavailable', 'cookies', 'expired',
                        'sign in', 'login', 'authentication', 'token', 'session', 'captcha'
                    ]):
                        mark_cookie_as_blocked(cookie_file, f"Ø®Ø·Ø£: {str(e)[:50]}")
                        LOGGER(__name__).warning(f"ğŸš« ØªÙ… Ø­Ø¸Ø± Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø¨Ø³Ø¨Ø¨: {str(e)[:50]}")
                    
                    if i < len(ydl_configs):
                        LOGGER(__name__).info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©...")
                        continue
                    else:
                        LOGGER(__name__).error(f"âŒ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„")
            
            LOGGER(__name__).error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ù…Ø­Ù…Ù„")
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pytube
            LOGGER(__name__).info("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pytube")
            try:
                from pytube import YouTube
                
                yt = YouTube(f"https://www.youtube.com/watch?v={video_id}")
                audio_stream = yt.streams.filter(only_audio=True).first()
                
                if audio_stream:
                    output_file = audio_stream.download(
                        output_path=str(temp_dir),
                        filename=f"{video_id}_pytube.mp4"
                    )
                    
                    if output_file and os.path.exists(output_file):
                        LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pytube: {output_file}")
                        return {
                            'success': True,
                            'file_path': output_file,
                            'title': yt.title or title,
                            'duration': yt.length or 0,
                            'uploader': yt.author or 'Unknown',
                            'elapsed': time.time() - start_time
                        }
                        
            except Exception as pytube_error:
                LOGGER(__name__).error(f"âŒ ÙØ´Ù„ pytube Ø£ÙŠØ¶Ø§Ù‹: {pytube_error}")
            
            return None
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: {e}")
            return None

    async def download_without_cookies(self, video_info: Dict) -> Optional[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙˆÙ† ÙƒÙˆÙƒÙŠØ² - Ù†Ø³Ø®Ø© Ù…Ø¨Ø³Ø·Ø© ÙˆØ³Ø±ÙŠØ¹Ø©"""
        if not yt_dlp:
            return None
            
        video_id = video_info.get("video_id")
        if not video_id:
            return None
        
        task_id = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
        self.active_tasks.add(task_id)
        start_time = time.time()
        
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø³Ø±ÙŠØ¹Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
            opts = {
                'format': 'worstaudio[ext=m4a]/worstaudio',
                'outtmpl': f'downloads/{video_id}_fallback.%(ext)s',
                'quiet': True,
                'no_warnings': True,
                'ignoreerrors': True,
                'extract_flat': False,
                'writethumbnail': False,
                'writeinfojson': False,
                'user_agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X)',
                'referer': 'https://www.google.com/',
                'timeout': 15,
                'retries': 1,
                'fragment_retries': 1,
                'skip_unavailable_fragments': True,
                'noprogress': True,
                'socket_timeout': 10,
                'force_generic_extractor': True,
            }
            
            url = f"https://youtu.be/{video_id}"
            
            loop = asyncio.get_running_loop()
            info = await loop.run_in_executor(
                self.conn_manager.executor_pool,
                lambda: yt_dlp.YoutubeDL(opts).extract_info(url, download=True)
            )
            
            if info:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„
                possible_files = [
                    f"downloads/{video_id}_fallback.m4a",
                    f"downloads/{video_id}_fallback.mp3",
                    f"downloads/{video_id}_fallback.webm"
                ]
                
                for audio_path in possible_files:
                    if os.path.exists(audio_path):
                        return {
                            "audio_path": audio_path,
                            "title": info.get("title", video_info.get("title", ""))[:60],
                            "artist": info.get("uploader", video_info.get("artist", "Unknown")),
                            "duration": int(info.get("duration", 0)),
                            "file_size": os.path.getsize(audio_path),
                            "source": "ytdlp_simple_fallback",
                            "elapsed": time.time() - start_time
                        }
                        
        except Exception as e:
            LOGGER(__name__).error(f"ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙˆÙ† ÙƒÙˆÙƒÙŠØ²: {e}")
            # self.monitor.log_error('fallback_download')
        finally:
            self.active_tasks.discard(task_id)
            
        return None

# Ù†Ø¸Ø§Ù… ØªØªØ¨Ø¹ Ø­Ø§Ù„Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²
COOKIES_STATUS = {}
BLOCKED_COOKIES = set()
COOKIES_USAGE_COUNT = {}
LAST_COOKIE_USED = None

# Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
PARALLEL_SEARCH_STATS = {
    'database_wins': 0,
    'smart_cache_wins': 0,
    'total_searches': 0,
    'avg_database_time': 0,
    'avg_smart_cache_time': 0,
    'database_times': [],
    'smart_cache_times': []
}

# Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ù…ÙˆÙ„Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ù…ÙˆÙ„Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© (Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡)
MAX_CONCURRENT_DOWNLOADS = 20          # Ø­Ø¯ Ù…Ø¹Ù‚ÙˆÙ„ Ù„Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
MAX_CONCURRENT_SEARCHES = 30           # Ø­Ø¯ Ù…Ø¹Ù‚ÙˆÙ„ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
MAX_QUEUE_SIZE = float('inf')          # Ù„Ø§ Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·Ø§Ø¨ÙˆØ±
RATE_LIMIT_WINDOW = 60                  # Ù†Ø§ÙØ²Ø© Ø²Ù…Ù†ÙŠØ© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
MAX_REQUESTS_PER_WINDOW = 1000          # Ø­Ø¯ Ù…Ø±Ù† Ù„Ù„Ø·Ù„Ø¨Ø§Øª (Ù…Ø¶Ø§Ø¹Ù)

# Ø£Ø¯ÙˆØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ (Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯)
# download_semaphore = None  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ¯
# search_semaphore = None    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ¯
thread_pool = ThreadPoolExecutor(max_workers=100)  # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠÙˆØ·

# ØªØªØ¨Ø¹ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª (Ù…Ø±Ù†)
request_times = defaultdict(lambda: deque(maxlen=MAX_REQUESTS_PER_WINDOW))
active_downloads = {}
# download_queue = asyncio.Queue()  # Ø·Ø§Ø¨ÙˆØ± Ø¨Ù„Ø§ Ø­Ø¯ÙˆØ¯ (Ù„Ù† Ù†Ø­ØªØ§Ø¬Ù‡)

# Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
PERFORMANCE_STATS = {
    'total_requests': 0,
    'successful_downloads': 0,
    'failed_downloads': 0,
    'cache_hits': 0,
    'avg_response_time': 0,
    'peak_concurrent': 0,
    'current_concurrent': 0,
    'queue_size': 0,
    'rate_limited': 0
}

async def check_rate_limit(user_id: int) -> bool:
    """ÙØ­Øµ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø±Ù†)"""
    current_time = time.time()
    user_requests = request_times[user_id]
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    while user_requests and current_time - user_requests[0] > RATE_LIMIT_WINDOW:
        user_requests.popleft()
    
    # ÙØ­Øµ Ù…Ø±Ù† - ØªØ­Ø°ÙŠØ± ÙÙ‚Ø· Ø¹Ù†Ø¯ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­
    if len(user_requests) >= MAX_REQUESTS_PER_WINDOW:
        PERFORMANCE_STATS['rate_limited'] += 1
        # ØªØ³Ø¬ÙŠÙ„ ØªØ­Ø°ÙŠØ± Ù„ÙƒÙ† Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©
        LOGGER(__name__).warning(f"âš ï¸ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­: {len(user_requests)} Ø·Ù„Ø¨ ÙÙŠ {RATE_LIMIT_WINDOW}s")
        
        # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù„Ù„Ø­Ù…ÙˆÙ„Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
        # return False  # Ù…Ø¹Ø·Ù„ - Ù„Ø§ Ø­Ø¯ÙˆØ¯ ØµØ§Ø±Ù…Ø©
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø­Ø§Ù„ÙŠ
    user_requests.append(current_time)
    return True  # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¯Ø§Ø¦Ù…Ø§Ù‹

async def update_performance_stats(success: bool, response_time: float, from_cache: bool = False):
    """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    PERFORMANCE_STATS['total_requests'] += 1
    
    if success:
        PERFORMANCE_STATS['successful_downloads'] += 1
    else:
        PERFORMANCE_STATS['failed_downloads'] += 1
    
    if from_cache:
        PERFORMANCE_STATS['cache_hits'] += 1
    
    # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
    current_avg = PERFORMANCE_STATS['avg_response_time']
    total_requests = PERFORMANCE_STATS['total_requests']
    PERFORMANCE_STATS['avg_response_time'] = ((current_avg * (total_requests - 1)) + response_time) / total_requests
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø±ÙˆØ©
    current_concurrent = len(active_downloads)
    PERFORMANCE_STATS['current_concurrent'] = current_concurrent
    if current_concurrent > PERFORMANCE_STATS['peak_concurrent']:
        PERFORMANCE_STATS['peak_concurrent'] = current_concurrent
    
    PERFORMANCE_STATS['queue_size'] = 0  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø·Ø§Ø¨ÙˆØ± - Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ©

def log_performance_stats():
    """ØªØ³Ø¬ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    stats = PERFORMANCE_STATS
    success_rate = (stats['successful_downloads'] / max(stats['total_requests'], 1)) * 100
    cache_hit_rate = (stats['cache_hits'] / max(stats['total_requests'], 1)) * 100
    
    LOGGER(__name__).info(
        f"ğŸ“Š Ø§Ù„Ø£Ø¯Ø§Ø¡: {stats['total_requests']} Ø·Ù„Ø¨ | "
        f"Ù†Ø¬Ø§Ø­: {success_rate:.1f}% | "
        f"ÙƒØ§Ø´: {cache_hit_rate:.1f}% | "
        f"Ù…ØªÙˆØ³Ø·: {stats['avg_response_time']:.2f}s | "
        f"Ù…ØªÙˆØ§Ø²ÙŠ: {stats['current_concurrent']}/{stats['peak_concurrent']} | "
        f"Ø·Ø§Ø¨ÙˆØ±: {stats['queue_size']}"
    )

async def process_unlimited_download(event, user_id: int, start_time: float):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„ÙÙˆØ±ÙŠ"""
    task_id = f"{user_id}_{int(time.time() * 1000000)}"  # Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
    
    try:
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙˆØ±Ø§Ù‹
        active_downloads[task_id] = {
            'user_id': user_id,
            'start_time': start_time,
            'task_id': task_id,
            'status': 'started'
        }
        
        LOGGER(__name__).info(f"ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} | Ø§Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙÙŠ Ù…Ù‡Ù…Ø© Ù…Ù†ÙØµÙ„Ø©
        await execute_parallel_download(event, user_id, start_time, task_id)
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ: {e}")
        await update_performance_stats(False, time.time() - start_time)
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù‡Ù…Ø©
        if task_id in active_downloads:
            active_downloads[task_id]['status'] = 'completed'
            del active_downloads[task_id]

async def execute_parallel_download(event, user_id: int, start_time: float, task_id: str):
    """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„"""
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        match = event.pattern_match
        if not match:
            await event.reply("âŒ **Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨**")
            return
        
        query = match.group(2) if match.group(2) else ""
        if not query:
            await event.reply("ğŸ“ **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** `Ø¨Ø­Ø« Ø§Ø³Ù… Ø§Ù„Ø£ØºÙ†ÙŠØ©`")
            await update_performance_stats(False, time.time() - start_time)
            return
        
        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
        if task_id in active_downloads:
            active_downloads[task_id].update({
                'query': query,
                'status': 'processing'
            })
        
        LOGGER(__name__).info(f"ğŸµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©: {query} | Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_id} | Ø§Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±Ø©
        await process_normal_download(event, query, user_id, start_time)
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ: {e}")
        await event.reply("âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ**")
        await update_performance_stats(False, time.time() - start_time)

async def process_normal_download(event, query: str, user_id: int, start_time: float):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
    bot_client = event.client
    
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø±Ø³Ù„ Ù„ÙŠØ³ Ø¨ÙˆØª
        if event.sender.bot:
            return
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ØªÙˆÙØ±Ø§Ù‹
        if not query:
            match = event.pattern_match
            if not match:
                return
            
            query = match.group(2) if match.group(2) else ""
            if not query:
                await event.reply("ğŸ“ **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** `Ø¨Ø­Ø« Ø§Ø³Ù… Ø§Ù„Ø£ØºÙ†ÙŠØ©`")
                await update_performance_stats(False, time.time() - start_time)
                return
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø©
        status_msg = await event.reply("ğŸ” **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ...**")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯
        parallel_result = await parallel_search_with_monitoring(query, bot_client)
        
        if parallel_result and parallel_result.get('success'):
            search_source = parallel_result.get('search_source', 'unknown')
            search_time = parallel_result.get('search_time', 0)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            await update_performance_stats(True, time.time() - start_time, from_cache=True)
            
            if search_source == 'database':
                await status_msg.edit(f"ğŸ“¤ **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ({search_time:.2f}s) - Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                await send_cached_from_database(event, status_msg, parallel_result, bot_client)
                return
            elif search_source == 'smart_cache':
                await status_msg.edit(f"ğŸ“¤ **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ ({search_time:.2f}s) - Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                await send_cached_audio(event, status_msg, parallel_result, bot_client)
                return
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ†ØŒ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
        await status_msg.edit("ğŸ” **Ù„Ù… ÙŠÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† - Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙŠÙˆØªÙŠÙˆØ¨...**")
        
        # Ù‡Ù†Ø§ ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¨Ø§Ù‚ÙŠ Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
        # (Ø³ÙŠØªÙ… Ø±Ø¨Ø·Ù‡ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        await update_performance_stats(True, time.time() - start_time)
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©: {e}")
        await update_performance_stats(False, time.time() - start_time)
        
        try:
            await status_msg.edit("âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©**")
        except:
            pass

def get_available_cookies():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ø¹ ØªØ¯ÙˆÙŠØ± Ø°ÙƒÙŠ"""
    try:
        import glob
        import os
        cookies_pattern = "cookies/cookies*.txt"
        all_cookies_files = glob.glob(cookies_pattern)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø©
        available_cookies = []
        for cookie_file in all_cookies_files:
            if cookie_file not in BLOCKED_COOKIES:
                available_cookies.append(cookie_file)
        
        if not available_cookies:
            LOGGER(__name__).warning("âš ï¸ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù…Ø­Ø¸ÙˆØ±Ø©! Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†...")
            BLOCKED_COOKIES.clear()
            available_cookies = all_cookies_files
        
        # ØªØ±ØªÙŠØ¨ Ø°ÙƒÙŠ: Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹
        available_cookies.sort(key=lambda x: (
            COOKIES_USAGE_COUNT.get(x, 0),  # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            os.path.getmtime(x)  # ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
        ))
        
        LOGGER(__name__).info(f"ğŸª Ù…ØªØ§Ø­: {len(available_cookies)} | Ù…Ø­Ø¸ÙˆØ±: {len(BLOCKED_COOKIES)} Ù…Ù„Ù ÙƒÙˆÙƒÙŠØ²")
        return available_cookies
    except Exception as e:
        LOGGER(__name__).warning(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {e}")
        return []

def mark_cookie_as_blocked(cookie_file: str, reason: str = "Ø­Ø¸Ø±"):
    """ØªÙ…ÙŠÙŠØ² Ù…Ù„Ù ÙƒÙˆÙƒÙŠØ² ÙƒÙ…Ø­Ø¸ÙˆØ± ÙˆØ­Ø°ÙÙ‡"""
    try:
        BLOCKED_COOKIES.add(cookie_file)
        LOGGER(__name__).warning(f"ğŸš« ØªÙ… Ø­Ø¸Ø± Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {os.path.basename(cookie_file)} - {reason}")
        
        # Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø°Ù
        backup_name = f"{cookie_file}.blocked_{int(time.time())}"
        if os.path.exists(cookie_file):
            os.rename(cookie_file, backup_name)
            LOGGER(__name__).info(f"ğŸ’¾ ØªÙ… Ù†Ø³Ø® Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…Ø­Ø¸ÙˆØ± Ø¥Ù„Ù‰: {os.path.basename(backup_name)}")
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        if cookie_file in COOKIES_USAGE_COUNT:
            del COOKIES_USAGE_COUNT[cookie_file]
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø¸Ø± Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {e}")

def track_cookie_usage(cookie_file: str, success: bool = True):
    """ØªØªØ¨Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø§Ù„ÙƒÙˆÙƒÙŠØ²"""
    global LAST_COOKIE_USED
    
    COOKIES_USAGE_COUNT[cookie_file] = COOKIES_USAGE_COUNT.get(cookie_file, 0) + 1
    LAST_COOKIE_USED = cookie_file
    
    status = "âœ…" if success else "âŒ"
    usage_count = COOKIES_USAGE_COUNT[cookie_file]
    
    LOGGER(__name__).info(f"{status} ÙƒÙˆÙƒÙŠØ²: {os.path.basename(cookie_file)} (Ø§Ø³ØªØ®Ø¯Ø§Ù… #{usage_count})")

def get_next_cookie_with_rotation():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ø¹ ØªØ¯ÙˆÙŠØ± Ø°ÙƒÙŠ"""
    available_cookies = get_available_cookies()
    
    if not available_cookies:
        return None
    
    # ØªØ¬Ù†Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø¤Ø®Ø±Ø§Ù‹
    if LAST_COOKIE_USED and len(available_cookies) > 1:
        try:
            available_cookies.remove(LAST_COOKIE_USED)
        except ValueError:
            pass
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
    next_cookie = available_cookies[0]
    LOGGER(__name__).info(f"ğŸ”„ ØªØ¯ÙˆÙŠØ± Ø¥Ù„Ù‰ ÙƒÙˆÙƒÙŠØ²: {os.path.basename(next_cookie)}")
    
    return next_cookie

def cleanup_blocked_cookies():
    """ØªÙ†Ø¸ÙŠÙ Ø¯ÙˆØ±ÙŠ Ù„Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø©"""
    try:
        import glob
        # Ø¥Ø°Ø§ ØªÙ… Ø­Ø¸Ø± Ø£ÙƒØ«Ø± Ù…Ù† 70% Ù…Ù† Ø§Ù„ÙƒÙˆÙƒÙŠØ²ØŒ Ø§Ø¹Ø¯ ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…
        total_cookies = len(glob.glob("cookies/cookies*.txt"))
        blocked_count = len(BLOCKED_COOKIES)
        
        if total_cookies > 0 and (blocked_count / total_cookies) > 0.7:
            LOGGER(__name__).warning(f"âš ï¸ ØªÙ… Ø­Ø¸Ø± {blocked_count}/{total_cookies} ÙƒÙˆÙƒÙŠØ² - Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…")
            BLOCKED_COOKIES.clear()
            COOKIES_USAGE_COUNT.clear()
            
        # Ø­Ø°Ù Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (Ø£ÙƒØ«Ø± Ù…Ù† 24 Ø³Ø§Ø¹Ø©)
        import time
        current_time = time.time()
        
        for backup_file in glob.glob("cookies/*.blocked_*"):
            try:
                file_time = os.path.getmtime(backup_file)
                if current_time - file_time > 86400:  # 24 Ø³Ø§Ø¹Ø©
                    os.remove(backup_file)
                    LOGGER(__name__).info(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: {os.path.basename(backup_file)}")
            except Exception as e:
                LOGGER(__name__).warning(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
                
        LOGGER(__name__).info(f"ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙˆÙƒÙŠØ²: Ù…ØªØ§Ø­={total_cookies-blocked_count} | Ù…Ø­Ø¸ÙˆØ±={blocked_count}")
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {e}")

def calculate_cookies_distribution(total_count: int) -> Dict[str, int]:
    """Ø­Ø³Ø§Ø¨ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø¨Ø´ÙƒÙ„ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ"""
    if total_count == 0:
        return {'primary': 0, 'secondary': 0, 'remaining': 0}
    
    # ØªÙˆØ²ÙŠØ¹ Ø°ÙƒÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
    if total_count <= 5:
        # Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        return {'primary': total_count, 'secondary': 0, 'remaining': 0}
    
    elif total_count <= 10:
        # Ø¹Ø¯Ø¯ Ù…ØªÙˆØ³Ø·: Ù‚Ø³Ù… Ø¨ÙŠÙ† Ø£Ø³Ø§Ø³ÙŠ ÙˆØ«Ø§Ù†ÙˆÙŠ
        primary = total_count // 2
        secondary = total_count - primary
        return {'primary': primary, 'secondary': secondary, 'remaining': 0}
    
    elif total_count <= 20:
        # Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ±: ØªÙˆØ²ÙŠØ¹ Ù…ØªÙˆØ§Ø²Ù†
        primary = max(4, total_count // 3)
        secondary = max(3, total_count // 4)
        remaining = total_count - primary - secondary
        return {'primary': primary, 'secondary': secondary, 'remaining': remaining}
    
    else:
        # Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹: ØªÙˆØ²ÙŠØ¹ Ù…Ø­Ø¯ÙˆØ¯ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥ÙØ±Ø§Ø·
        primary = min(8, max(5, total_count // 4))
        secondary = min(6, max(4, total_count // 5))
        remaining = min(10, total_count - primary - secondary)
        return {'primary': primary, 'secondary': secondary, 'remaining': remaining}

def get_cookies_statistics():
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù…Ø¹ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ"""
    try:
        import glob
        total_cookies = len(glob.glob("cookies/cookies*.txt"))
        available_cookies = len(get_available_cookies())
        blocked_cookies = len(BLOCKED_COOKIES)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        distribution = calculate_cookies_distribution(available_cookies)
        
        # Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        most_used = max(COOKIES_USAGE_COUNT.items(), key=lambda x: x[1]) if COOKIES_USAGE_COUNT else ("Ù„Ø§ ÙŠÙˆØ¬Ø¯", 0)
        
        stats = {
            'total': total_cookies,
            'available': available_cookies, 
            'blocked': blocked_cookies,
            'distribution': distribution,
            'most_used_file': os.path.basename(most_used[0]) if most_used[0] != "Ù„Ø§ ÙŠÙˆØ¬Ø¯" else "Ù„Ø§ ÙŠÙˆØ¬Ø¯",
            'most_used_count': most_used[1],
            'usage_distribution': dict(COOKIES_USAGE_COUNT)
        }
        
        return stats
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {e}")
        return {}

async def parallel_search_with_monitoring(query: str, bot_client) -> Optional[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    start_time = time.time()
    
    try:
        LOGGER(__name__).info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ: {query}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„ÙˆÙ‚Øª
        db_task = asyncio.create_task(search_in_database_cache(query))
        cache_task = asyncio.create_task(search_in_telegram_cache(query, bot_client))
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
        db_start = time.time()
        cache_start = time.time()
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø£ÙˆÙ„ Ù†ØªÙŠØ¬Ø© Ù†Ø§Ø¬Ø­Ø©
        done, pending = await asyncio.wait(
            [db_task, cache_task], 
            return_when=asyncio.FIRST_COMPLETED,
            timeout=10  # Ù…Ù‡Ù„Ø© Ø²Ù…Ù†ÙŠØ© 10 Ø«ÙˆØ§Ù†
        )
        
        # ÙØ­Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        for task in done:
            try:
                result = await task
                if result and result.get('success'):
                    elapsed = time.time() - start_time
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    PARALLEL_SEARCH_STATS['total_searches'] += 1
                    
                    if task == db_task:
                        LOGGER(__name__).info(f"ğŸ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø²Øª! ({elapsed:.2f}s)")
                        result['search_source'] = 'database'
                        result['search_time'] = elapsed
                        
                        # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        PARALLEL_SEARCH_STATS['database_wins'] += 1
                        PARALLEL_SEARCH_STATS['database_times'].append(elapsed)
                        if len(PARALLEL_SEARCH_STATS['database_times']) > 100:
                            PARALLEL_SEARCH_STATS['database_times'].pop(0)
                        PARALLEL_SEARCH_STATS['avg_database_time'] = sum(PARALLEL_SEARCH_STATS['database_times']) / len(PARALLEL_SEARCH_STATS['database_times'])
                        
                    elif task == cache_task:
                        LOGGER(__name__).info(f"ğŸ† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ ÙØ§Ø²! ({elapsed:.2f}s)")
                        result['search_source'] = 'smart_cache'
                        result['search_time'] = elapsed
                        
                        # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ
                        PARALLEL_SEARCH_STATS['smart_cache_wins'] += 1
                        PARALLEL_SEARCH_STATS['smart_cache_times'].append(elapsed)
                        if len(PARALLEL_SEARCH_STATS['smart_cache_times']) > 100:
                            PARALLEL_SEARCH_STATS['smart_cache_times'].pop(0)
                        PARALLEL_SEARCH_STATS['avg_smart_cache_time'] = sum(PARALLEL_SEARCH_STATS['smart_cache_times']) / len(PARALLEL_SEARCH_STATS['smart_cache_times'])
                    
                    # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
                    for pending_task in pending:
                        pending_task.cancel()
                    
                    return result
                    
            except Exception as e:
                LOGGER(__name__).warning(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¨Ø­Ø«: {e}")
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙ†Ø¬Ø­ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©ØŒ Ø§Ù†ØªØ¸Ø± Ø§Ù„Ø¨Ø§Ù‚ÙŠ
        if pending:
            LOGGER(__name__).info("â³ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©...")
            try:
                remaining_results = await asyncio.gather(*pending, return_exceptions=True)
                
                for i, result in enumerate(remaining_results):
                    if isinstance(result, Exception):
                        continue
                        
                    if result and result.get('success'):
                        elapsed = time.time() - start_time
                        remaining_tasks = list(pending)
                        
                        if remaining_tasks[i] == db_task:
                            LOGGER(__name__).info(f"âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø¬Ø­Øª (Ù…ØªØ£Ø®Ø±Ø©: {elapsed:.2f}s)")
                            result['search_source'] = 'database'
                        elif remaining_tasks[i] == cache_task:
                            LOGGER(__name__).info(f"âœ… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ù†Ø¬Ø­ (Ù…ØªØ£Ø®Ø±: {elapsed:.2f}s)")
                            result['search_source'] = 'smart_cache'
                        
                        result['search_time'] = elapsed
                        return result
                        
            except asyncio.TimeoutError:
                LOGGER(__name__).warning("â° Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ")
        
        total_time = time.time() - start_time
        LOGGER(__name__).info(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ ({total_time:.2f}s)")
        return None
        
    except Exception as e:
        total_time = time.time() - start_time
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ ({total_time:.2f}s): {e}")
        return None

# === Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ© ===

async def search_in_database_cache(query: str) -> Optional[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ© (Ø§Ù„ÙƒØ§Ø´)"""
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù„Ù„Ø¨Ø­Ø«
        normalized_query = normalize_search_text(query)
        search_keywords = normalized_query.split()
        
        LOGGER(__name__).info(f"ğŸ—„ï¸ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: '{normalized_query}' (ÙƒÙ„Ù…Ø§Øª: {search_keywords})")
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        search_conditions = []
        search_params = []
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø£ÙˆÙ„Ø§Ù‹
        search_conditions.append("(title_normalized LIKE ? OR artist_normalized LIKE ?)")
        search_params.extend([f"%{normalized_query}%", f"%{normalized_query}%"])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        arabic_variants = {
            'ÙˆØ­Ø´ØªÙ†ÙŠ': ['ÙˆØ­Ø´ØªÙ†ÙŠ', 'ÙˆØ­Ø´ØªÙŠÙ†ÙŠ', 'ÙˆØ­Ø´Ù†ÙŠ', 'ÙˆØ­Ø´ØªÙ†Ù‰'],
            'Ø§Ø­Ø¨Ùƒ': ['Ø§Ø­Ø¨Ùƒ', 'Ø£Ø­Ø¨Ùƒ', 'Ø§Ø­Ø¨Ù‘Ùƒ', 'Ø£Ø­Ø¨Ù‘Ùƒ'],
            'Ø­Ø¨ÙŠØ¨ÙŠ': ['Ø­Ø¨ÙŠØ¨ÙŠ', 'Ø­Ø¨ÙŠØ¨Ù‰'],
            'Ø¹Ù„ÙŠÙƒ': ['Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙƒÙŠ'],
            'Ø§Ù†Øª': ['Ø§Ù†Øª', 'Ø£Ù†Øª', 'Ø¥Ù†Øª']
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        for original_word in search_keywords:
            if len(original_word) > 2:
                variants = arabic_variants.get(original_word, [original_word])
                for variant in variants:
                    search_conditions.append("(title_normalized LIKE ? OR artist_normalized LIKE ? OR keywords_vector LIKE ?)")
                    search_params.extend([f"%{variant}%", f"%{variant}%", f"%{variant}%"])
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø§Ù… Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØ±Ø¯Ø© (Ø§Ø­ØªÙŠØ§Ø·ÙŠ)
        for keyword in search_keywords:
            if len(keyword) > 2:
                search_conditions.append("(title_normalized LIKE ? OR artist_normalized LIKE ? OR keywords_vector LIKE ?)")
                search_params.extend([f"%{keyword}%", f"%{keyword}%", f"%{keyword}%"])
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© ÙˆØ¢Ø®Ø± ÙˆØµÙˆÙ„
        query_sql = f"""
        SELECT message_id, file_id, file_unique_id, original_title, original_artist, 
               duration, file_size, access_count, last_accessed, popularity_rank,
               title_normalized, artist_normalized
        FROM channel_index 
        WHERE ({' OR '.join(search_conditions)})
        ORDER BY popularity_rank DESC, access_count DESC, last_accessed DESC
        LIMIT 5
        """
        
        cursor.execute(query_sql, search_params)
        results = cursor.fetchall()
        
        LOGGER(__name__).info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        if results:
            # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
            best_result = results[0]
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
            cursor.execute("""
                UPDATE channel_index 
                SET access_count = access_count + 1, 
                    last_accessed = CURRENT_TIMESTAMP,
                    popularity_rank = popularity_rank + 0.1
                WHERE message_id = ?
            """, (best_result[0],))
            
            conn.commit()
            conn.close()
            
            # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
            title_words = set(best_result[10].split())  # title_normalized
            artist_words = set(best_result[11].split())  # artist_normalized
            query_words = set(search_keywords)
            
            all_content_words = title_words | artist_words
            match_ratio = len(query_words & all_content_words) / len(query_words) if query_words else 0
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ·Ø§Ø¨Ù‚ (80% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)
            MIN_MATCH_RATIO = 0.8
            if match_ratio < MIN_MATCH_RATIO:
                LOGGER(__name__).info(f"âŒ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹: {match_ratio:.1%} (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰: {MIN_MATCH_RATIO:.1%})")
                conn.close()
                return None
            
            LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù‚ÙˆÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {match_ratio:.1%}")
            
            return {
                'success': True,
                'cached': True,
                'from_database': True,
                'message_id': best_result[0],
                'file_id': best_result[1],
                'file_unique_id': best_result[2],
                'title': best_result[3],  # original_title
                'uploader': best_result[4],  # original_artist
                'duration': best_result[5],
                'file_size': best_result[6],
                'access_count': best_result[7] + 1,
                'match_ratio': match_ratio
            }
        
        conn.close()
        LOGGER(__name__).info("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return None

async def send_cached_from_database(event, status_msg, db_result: Dict, bot_client):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… file_id"""
    try:
        import config
        
        LOGGER(__name__).info(f"ğŸ“¤ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {db_result.get('title', 'Unknown')}")
        await status_msg.edit("ğŸ“¤ **Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ...**")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©
        duration = db_result.get('duration', 0)
        duration_str = f"{duration//60}:{duration%60:02d}" if duration > 0 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        user_caption = f"âœ¦ @{config.BOT_USERNAME}"
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
        thumb_path = None
        try:
            title = db_result.get('title', 'Unknown')
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
            if 'message_id' in db_result and bot_client:
                try:
                    if hasattr(config, 'CACHE_CHANNEL_ID') and config.CACHE_CHANNEL_ID:
                        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                        channel_msg = await bot_client.get_messages(config.CACHE_CHANNEL_ID, ids=db_result['message_id'])
                        if channel_msg and channel_msg.media and hasattr(channel_msg.media, 'document'):
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ù
                            if hasattr(channel_msg.media.document, 'thumbs') and channel_msg.media.document.thumbs:
                                thumb_path = channel_msg.media.document.thumbs[0]
                                LOGGER(__name__).info(f"ğŸ“¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†")
                except Exception as thumb_error:
                    LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {thumb_error}")
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {e}")
        
        LOGGER(__name__).info(f"ğŸ“‹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø±Ø³Ø§Ù„: file_id={db_result['file_id'][:20]}..., duration={duration}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù ÙƒØ±Ø¯ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        sent_message = await event.reply(
            user_caption,
            file=db_result['file_id'],
            thumb=thumb_path,
            attributes=[
                DocumentAttributeAudio(
                    duration=duration,
                    title=db_result.get('title', 'Unknown')[:60],
                    performer=db_result.get('uploader', 'Unknown')[:40]
                )
            ]
        )
        
        await status_msg.delete()
        LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ±Ø¯ Ø¨Ù†Ø¬Ø§Ø­: {sent_message.id}")
        return True
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© - Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„
        return False

async def send_cached_from_telegram(event, status_msg, cache_result: Dict, bot_client):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ (Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†)"""
    try:
        import config
        
        LOGGER(__name__).info(f"ğŸ“¤ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ: {cache_result.get('title', 'Unknown')}")
        await status_msg.edit("ğŸ“¤ **Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ...**")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©
        duration = cache_result.get('duration', 0)
        user_caption = f"âœ¦ @{config.BOT_USERNAME}"
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
        thumb_path = None
        try:
            if 'message_id' in cache_result and bot_client:
                try:
                    if hasattr(config, 'CACHE_CHANNEL_ID') and config.CACHE_CHANNEL_ID:
                        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                        channel_msg = await bot_client.get_messages(config.CACHE_CHANNEL_ID, ids=cache_result['message_id'])
                        if channel_msg and channel_msg.media and hasattr(channel_msg.media, 'document'):
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ù
                            if hasattr(channel_msg.media.document, 'thumbs') and channel_msg.media.document.thumbs:
                                thumb_path = channel_msg.media.document.thumbs[0]
                                LOGGER(__name__).info(f"ğŸ“¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†")
                except Exception as thumb_error:
                    LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {thumb_error}")
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {e}")
        
        LOGGER(__name__).info(f"ğŸ“‹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø±Ø³Ø§Ù„: file_id={cache_result['file_id'][:20]}..., duration={duration}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù ÙƒØ±Ø¯ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        sent_message = await event.reply(
            user_caption,
            file=cache_result['file_id'],
            thumb=thumb_path,
            attributes=[
                DocumentAttributeAudio(
                    duration=duration,
                    title=cache_result.get('title', 'Unknown')[:60],
                    performer=cache_result.get('uploader', 'Unknown')[:40]
                )
            ]
        )
        
        await status_msg.delete()
        LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ ÙƒØ±Ø¯ Ø¨Ù†Ø¬Ø§Ø­: {sent_message.id}")
        return True
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ: {e}")
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© - Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„
        return False

async def save_to_database_cache(file_id: str, file_unique_id: str, message_id: int, result: Dict, query: str) -> bool:
    """Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©"""
    try:
        # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        title = result.get('title', 'Unknown')
        artist = result.get('uploader', 'Unknown')
        duration = result.get('duration', 0)
        file_size = result.get('file_size', 0)
        
        title_normalized = normalize_search_text(title)
        artist_normalized = normalize_search_text(artist)
        
        # Ø¥Ù†Ø´Ø§Ø¡ vector Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords_vector = f"{title_normalized} {artist_normalized} {normalize_search_text(query)}"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ Ø§Ù„Ø¨Ø­Ø«
        search_hash = hashlib.md5((title_normalized + artist_normalized).encode()).hexdigest()
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø£Ùˆ ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©)
        cursor.execute("""
            INSERT OR REPLACE INTO channel_index 
            (message_id, file_id, file_unique_id, search_hash, title_normalized, 
             artist_normalized, keywords_vector, original_title, original_artist, 
             duration, file_size, access_count, popularity_rank)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, 1.0)
        """, (
            message_id, file_id, file_unique_id, search_hash,
            title_normalized, artist_normalized, keywords_vector,
            title, artist, duration, file_size
        ))
        
        conn.commit()
        conn.close()
        
        LOGGER(__name__).info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {title[:30]}")
        return True
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return False

# === Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… ===

async def search_in_telegram_cache(query: str, bot_client) -> Optional[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø®Ø§Ø±Ù‚ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù…Ø¹ ÙÙ‡Ø±Ø³Ø© Ù…ØªÙ‚Ø¯Ù…Ø©"""
    try:
        import config
        
        if not hasattr(config, 'CACHE_CHANNEL_ID') or not config.CACHE_CHANNEL_ID:
            LOGGER(__name__).warning("âŒ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©")
            return None
        
        cache_channel = config.CACHE_CHANNEL_ID
        LOGGER(__name__).info(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø®Ø§Ø±Ù‚ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ†: {cache_channel}")
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù„Ù„Ø¨Ø­Ø«
        normalized_query = normalize_search_text(query)
        search_keywords = normalized_query.split()
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹ (Ø£Ø³Ø±Ø¹)
        try:
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            
            # Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            search_conditions = []
            search_params = []
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø£ÙˆÙ„Ø§Ù‹
            search_conditions.append("(title_normalized LIKE ? OR artist_normalized LIKE ?)")
            search_params.extend([f"%{normalized_query}%", f"%{normalized_query}%"])
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            arabic_variants = {
                'ÙˆØ­Ø´ØªÙ†ÙŠ': ['ÙˆØ­Ø´ØªÙ†ÙŠ', 'ÙˆØ­Ø´ØªÙŠÙ†ÙŠ', 'ÙˆØ­Ø´Ù†ÙŠ', 'ÙˆØ­Ø´ØªÙ†Ù‰'],
                'Ø§Ø­Ø¨Ùƒ': ['Ø§Ø­Ø¨Ùƒ', 'Ø£Ø­Ø¨Ùƒ', 'Ø§Ø­Ø¨Ù‘Ùƒ', 'Ø£Ø­Ø¨Ù‘Ùƒ'],
                'Ø­Ø¨ÙŠØ¨ÙŠ': ['Ø­Ø¨ÙŠØ¨ÙŠ', 'Ø­Ø¨ÙŠØ¨Ù‰'],
                'Ø¹Ù„ÙŠÙƒ': ['Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙƒÙŠ'],
                'Ø§Ù†Øª': ['Ø§Ù†Øª', 'Ø£Ù†Øª', 'Ø¥Ù†Øª']
            }
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            for original_word in search_keywords:
                if len(original_word) > 2:
                    variants = arabic_variants.get(original_word, [original_word])
                    for variant in variants:
                        search_conditions.append("(title_normalized LIKE ? OR artist_normalized LIKE ? OR keywords_vector LIKE ?)")
                        search_params.extend([f"%{variant}%", f"%{variant}%", f"%{variant}%"])
            
            # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø­Ø³Ù† Ù…Ø¹ ØªØ±ØªÙŠØ¨ Ø°ÙƒÙŠ
            query_sql = f"""
            SELECT message_id, file_id, file_unique_id, original_title, original_artist, 
                   duration, file_size, access_count, last_accessed, popularity_rank,
                   title_normalized, artist_normalized, keywords_vector
            FROM channel_index 
            WHERE ({' OR '.join(search_conditions)})
            ORDER BY 
                -- Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
                CASE WHEN title_normalized LIKE '%{normalized_query}%' THEN 1 ELSE 2 END,
                -- Ø«Ù… Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
                popularity_rank DESC, 
                access_count DESC, 
                last_accessed DESC
            LIMIT 10
            """
            
            cursor.execute(query_sql, search_params)
            db_results = cursor.fetchall()
            
            if db_results:
                # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù„ÙƒÙ„ Ù†ØªÙŠØ¬Ø©
                best_match = None
                best_score = 0
                
                for result in db_results:
                    # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
                    title_words = set(result[10].split())  # title_normalized
                    artist_words = set(result[11].split())  # artist_normalized
                    keywords_words = set(result[12].split())  # keywords_vector
                    query_words = set(search_keywords)
                    
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
                    title_match = len(query_words & title_words) / len(query_words) if query_words else 0
                    artist_match = len(query_words & artist_words) / len(query_words) if query_words else 0
                    keywords_match = len(query_words & keywords_words) / len(query_words) if query_words else 0
                    
                    # Ø¯Ø±Ø¬Ø© Ù…Ø±ÙƒØ¨Ø© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†
                    composite_score = (
                        title_match * 0.5 +      # ÙˆØ²Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† 50%
                        artist_match * 0.3 +     # ÙˆØ²Ù† Ø§Ù„ÙÙ†Ø§Ù† 30%
                        keywords_match * 0.2     # ÙˆØ²Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© 20%
                    )
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø¨ÙˆÙ†Øµ Ù„Ù„Ø´Ø¹Ø¨ÙŠØ©
                    popularity_bonus = min(result[9] / 10, 0.1)  # Ø£Ù‚ØµÙ‰ Ø¨ÙˆÙ†Øµ 10%
                    composite_score += popularity_bonus
                    
                    if composite_score > best_score and composite_score > 0.8:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ 80%
                        best_score = composite_score
                        best_match = result
                
                if best_match:
                    # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
                    cursor.execute("""
                        UPDATE channel_index 
                        SET access_count = access_count + 1, 
                            last_accessed = CURRENT_TIMESTAMP,
                            popularity_rank = popularity_rank + 0.1
                        WHERE message_id = ?
                    """, (best_match[0],))
                    conn.commit()
                    
                    LOGGER(__name__).info(f"âœ… Ù…Ø·Ø§Ø¨Ù‚Ø© Ù‚ÙˆÙŠØ© ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ: {best_score:.1%}")
                    
                    conn.close()
                    return {
                        'success': True,
                        'cached': True,
                        'from_database': True,
                        'message_id': best_match[0],
                        'file_id': best_match[1],
                        'file_unique_id': best_match[2],
                        'title': best_match[3],
                        'uploader': best_match[4],
                        'duration': best_match[5],
                        'file_size': best_match[6],
                        'access_count': best_match[7] + 1,
                        'match_ratio': best_score
                    }
            
            conn.close()
            
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù‚ÙˆÙŠØ©
            if not best_match:
                LOGGER(__name__).info("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù‚ÙˆÙŠØ© ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰: 80%)")
            
        except Exception as db_error:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {db_error}")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ø¹Ø·Ù„ (Ù‚ÙŠÙˆØ¯ API Ù„Ù„Ø¨ÙˆØªØ§Øª)
        LOGGER(__name__).info("âš ï¸ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹Ø·Ù„ Ù…Ø¤Ù‚ØªØ§Ù‹ (Ù‚ÙŠÙˆØ¯ API Ù„Ù„Ø¨ÙˆØªØ§Øª)")
        return None  # ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ
        
        # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…ÙØ­ÙˆØµØ© Ø¥Ù„Ù‰ 500 Ø±Ø³Ø§Ù„Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        search_limit = 500
        batch_size = 50  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        
        best_matches = []
        processed_count = 0
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
        async for message in bot_client.iter_messages(cache_channel, limit=search_limit):
            if not (message.text and message.file):
                continue
                
            processed_count += 1
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            message_text = message.text.lower()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ
            title = extract_title_from_cache_text(message.text)
            uploader = extract_uploader_from_cache_text(message.text)
            duration = extract_duration_from_cache_text(message.text)
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
            title_normalized = normalize_search_text(title)
            uploader_normalized = normalize_search_text(uploader)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            title_words = set(title_normalized.split())
            uploader_words = set(uploader_normalized.split())
            message_words = set(normalize_search_text(message_text).split())
            query_words = set(search_keywords)
            
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªØ·Ø§Ø¨Ù‚
            title_match = len(query_words & title_words) / len(query_words) if query_words else 0
            uploader_match = len(query_words & uploader_words) / len(query_words) if query_words else 0
            message_match = len(query_words & message_words) / len(query_words) if query_words else 0
            
            # Ø¯Ø±Ø¬Ø© Ù…Ø±ÙƒØ¨Ø© Ù…Ø­Ø³Ù†Ø©
            composite_score = (
                title_match * 0.4 +        # ÙˆØ²Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† 40%
                uploader_match * 0.3 +     # ÙˆØ²Ù† Ø§Ù„ÙÙ†Ø§Ù† 30%
                message_match * 0.3        # ÙˆØ²Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ 30%
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ø¨ÙˆÙ†Øµ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
            age_bonus = min((search_limit - processed_count) / search_limit * 0.1, 0.1)
            composite_score += age_bonus
            
            if composite_score > 0.5:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ 50% Ù„Ù„ØªØ·Ø§Ø¨Ù‚
                best_matches.append({
                    'score': composite_score,
                    'message': message,
                    'title': title,
                    'uploader': uploader,
                    'duration': duration,
                    'message_id': message.id,
                    'file_id': message.file.id
                })
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
            if processed_count % batch_size == 0:
                # ØªØ±ØªÙŠØ¨ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†
                best_matches = sorted(best_matches, key=lambda x: x['score'], reverse=True)[:5]
                LOGGER(__name__).info(f"ğŸ”„ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} Ø±Ø³Ø§Ù„Ø©ØŒ Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚: {best_matches[0]['score']:.1%}" if best_matches else f"ğŸ”„ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} Ø±Ø³Ø§Ù„Ø©")
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
        if best_matches:
            best_matches = sorted(best_matches, key=lambda x: x['score'], reverse=True)
            best_result = best_matches[0]
            
            LOGGER(__name__).info(f"âœ… Ø£ÙØ¶Ù„ Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ù‚Ù†Ø§Ø©: {best_result['score']:.1%} Ù…Ù† {processed_count} Ø±Ø³Ø§Ù„Ø©")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©
            try:
                file_info = {
                    'title': best_result['title'],
                    'uploader': best_result['uploader'],
                    'duration': best_result['duration'],
                    'file_size': best_result['message'].file.size if best_result['message'].file.size else 0
                }
                
                await save_to_database_cache(
                    best_result['file_id'],
                    best_result['message'].file.unique_id,
                    best_result['message_id'],
                    file_info,
                    query
                )
                LOGGER(__name__).info("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹ Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹")
                
            except Exception as save_error:
                LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {save_error}")
            
            return {
                'success': True,
                'cached': True,
                'message_id': best_result['message_id'],
                'file_id': best_result['file_id'],
                'title': best_result['title'],
                'duration': best_result['duration'],
                'uploader': best_result['uploader'],
                'match_ratio': best_result['score'],
                'original_message': best_result['message'],
                'processed_messages': processed_count
            }
        
        LOGGER(__name__).info(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† {processed_count} Ø±Ø³Ø§Ù„Ø©")
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")
        return None

# ØªÙ… Ø¯Ù…Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù…Ø¹ normalize_arabic_text
normalize_search_text = normalize_arabic_text  # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ§Ù„

def extract_title_from_cache_text(text: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ù†Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    try:
        import re
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¨Ø¹Ø¯ ğŸµ
        title_match = re.search(r'ğŸµ\s*\*\*(.+?)\*\*', text)
        if title_match:
            return title_match.group(1).strip()
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¨Ø¹Ø¯ "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:"
        title_match = re.search(r'Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:\s*(.+?)(?:\n|$)', text)
        if title_match:
            return title_match.group(1).strip()
        
        return "Unknown Title"
    except:
        return "Unknown Title"

def extract_duration_from_cache_text(text: str) -> int:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¯Ø© Ù…Ù† Ù†Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    try:
        import re
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¯Ø© Ø¨ØµÙŠØºØ© mm:ss
        duration_match = re.search(r'â±ï¸\s*\*\*(\d+):(\d+)\*\*', text)
        if duration_match:
            minutes = int(duration_match.group(1))
            seconds = int(duration_match.group(2))
            return minutes * 60 + seconds
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¯Ø© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        duration_match = re.search(r'Ø§Ù„Ù…Ø¯Ø©:\s*(\d+)', text)
        if duration_match:
            return int(duration_match.group(1))
        
        return 0
    except:
        return 0

def extract_uploader_from_cache_text(text: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø±Ø§ÙØ¹ Ù…Ù† Ù†Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    try:
        import re
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙÙ†Ø§Ù† Ø¨Ø¹Ø¯ ğŸ¤
        uploader_match = re.search(r'ğŸ¤\s*\*\*(.+?)\*\*', text)
        if uploader_match:
            return uploader_match.group(1).strip()
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙÙ†Ø§Ù† Ø¨Ø¹Ø¯ "Ø§Ù„ÙÙ†Ø§Ù†:"
        uploader_match = re.search(r'Ø§Ù„ÙÙ†Ø§Ù†:\s*(.+?)(?:\n|$)', text)
        if uploader_match:
            return uploader_match.group(1).strip()
        
        return "Unknown Artist"
    except:
        return "Unknown Artist"

async def save_to_smart_cache(bot_client, file_path: str, result: Dict, query: str, thumb_path: str = None) -> bool:
    """Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ ÙÙ‡Ø±Ø³Ø© Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªÙØµÙŠÙ„ Ø´Ø§Ù…Ù„"""
    try:
        import config
        import os
        from pathlib import Path
        
        if not hasattr(config, 'CACHE_CHANNEL_ID') or not config.CACHE_CHANNEL_ID:
            LOGGER(__name__).warning("âŒ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø© - ØªØ®Ø·ÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ†")
            return False
        
        cache_channel = config.CACHE_CHANNEL_ID
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµÙ„Ø©
        title = result.get('title', 'Unknown')
        uploader = result.get('uploader', 'Unknown')
        duration = result.get('duration', 0)
        file_size = result.get('file_size', 0)
        source = result.get('source', 'Unknown')
        elapsed_time = result.get('elapsed', 0)
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø¯Ø©
        duration_str = f"{duration//60}:{duration%60:02d}" if duration > 0 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
        if file_size > 0:
            if file_size >= 1024*1024:
                size_str = f"{file_size/(1024*1024):.1f} MB"
            else:
                size_str = f"{file_size/1024:.1f} KB"
        else:
            size_str = "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
        title_normalized = normalize_search_text(title)
        uploader_normalized = normalize_search_text(uploader)
        query_normalized = normalize_search_text(query)
        
        # Ù‡Ø§Ø´ Ù…Ø±ÙƒØ¨ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
        search_data = f"{title_normalized}|{uploader_normalized}|{query_normalized}"
        search_hash = hashlib.md5(search_data.encode()).hexdigest()[:12]
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø´Ø§Ù…Ù„Ø©
        all_keywords = set()
        all_keywords.update(title_normalized.split())
        all_keywords.update(uploader_normalized.split())
        all_keywords.update(query_normalized.split())
        
        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© Ø°ÙƒÙŠØ©
        if 'Ø­Ø¨ÙŠØ¨ØªÙŠ' in query_normalized or 'Ø­Ø¨ÙŠØ¨ÙŠ' in query_normalized:
            all_keywords.add('Ø­Ø¨')
            all_keywords.add('Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ')
        if 'Ø§ØºÙ†ÙŠØ©' in query_normalized or 'Ø£ØºÙ†ÙŠØ©' in query_normalized:
            all_keywords.add('Ù…ÙˆØ³ÙŠÙ‚Ù‰')
            all_keywords.add('ØºÙ†Ø§Ø¡')
        
        keywords_vector = ' '.join(sorted(all_keywords))
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØµÙ„ ÙˆØ§Ù„Ø°ÙƒÙŠ Ù„Ù„ØªØ®Ø²ÙŠÙ†
        cache_text = f"""ğŸµ **{title[:80]}**
ğŸ¤ **{uploader[:50]}**
â±ï¸ **{duration_str}** ({duration}s) | ğŸ“Š **{size_str}**

ğŸ” **Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£ØµÙ„ÙŠ:** `{query[:100]}`
ğŸ·ï¸ **Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©:** `{keywords_vector[:200]}`
ğŸ”— **Ø§Ù„Ù…ØµØ¯Ø±:** {source}
âš¡ **ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„:** {elapsed_time:.1f}s

ğŸ“Š **Ù‡Ø§Ø´ Ø§Ù„Ø¨Ø­Ø«:** `{search_hash}`
ğŸ“… **ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ®Ø²ÙŠÙ†:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸ†” **Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù„Ù:** `{os.path.basename(file_path)}`

ğŸ¤– **Ø¨ÙˆØ§Ø³Ø·Ø©:** ZeMusic Smart Cache System V2
ğŸ”„ **Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹:** Ø§Ø³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø£Ø¹Ù„Ø§Ù‡

#ØªØ®Ø²ÙŠÙ†_Ø°ÙƒÙŠ #Ù…ÙˆØ³ÙŠÙ‚Ù‰ #ÙÙ‡Ø±Ø³Ø©_Ù…ØªÙ‚Ø¯Ù…Ø©
#{title_normalized.replace(' ', '_')[:30]} #{uploader_normalized.replace(' ', '_')[:20]}
#{query_normalized.replace(' ', '_')[:30]} #Ù‡Ø§Ø´_{search_hash}"""
        
        try:
            # Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø©
            import asyncio
            
            async def upload_to_storage():
                try:
                    sent_message = await bot_client.send_file(
                        cache_channel,
                        file_path,
                        caption=cache_text,
                        thumb=thumb_path,
                        attributes=[
                            DocumentAttributeAudio(
                                duration=duration,
                                title=title[:64],
                                performer=uploader[:64]
                            )
                        ],
                        supports_streaming=True,
                        force_document=False
                    )
                    return sent_message
                except Exception as e:
                    LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")
                    return None
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±ÙØ¹ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
            upload_task = asyncio.create_task(upload_to_storage())
            sent_message = await upload_task
            
            if thumb_path:
                LOGGER(__name__).info(f"âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {title[:30]}")
            else:
                LOGGER(__name__).info(f"âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {title[:30]}")
            
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if sent_message and sent_message.file:
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                enhanced_info = {
                    'title': title,
                    'uploader': uploader,
                    'duration': duration,
                    'file_size': sent_message.file.size or file_size,
                    'source': source,
                    'search_hash': search_hash,
                    'keywords_vector': keywords_vector,
                    'original_query': query,
                    'upload_time': datetime.now().isoformat()
                }
                
                # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø©
                async def save_to_db():
                    try:
                        success = await save_to_database_cache_enhanced(
                            sent_message.file.id,
                            getattr(sent_message.file, 'unique_id', None),
                            sent_message.id,
                            enhanced_info,
                            query
                        )
                        if success:
                            LOGGER(__name__).info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                        else:
                            LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                    except Exception as e:
                        LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
                
                # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
                asyncio.create_task(save_to_db())
            
            LOGGER(__name__).info(f"ğŸ¯ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ Ù…Ø¹ ÙÙ‡Ø±Ø³Ø© Ø´Ø§Ù…Ù„Ø©: {os.path.basename(file_path)}")
            return True
            
        except Exception as upload_error:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù: {upload_error}")
            return False
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
        return False

async def save_to_database_cache_enhanced(file_id: str, file_unique_id: str, message_id: int, enhanced_info: Dict, query: str) -> bool:
    """Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©"""
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        title = enhanced_info.get('title', 'Unknown')
        artist = enhanced_info.get('uploader', 'Unknown')
        duration = enhanced_info.get('duration', 0)
        file_size = enhanced_info.get('file_size', 0)
        source = enhanced_info.get('source', 'Unknown')
        search_hash = enhanced_info.get('search_hash', '')
        keywords_vector = enhanced_info.get('keywords_vector', '')
        original_query = enhanced_info.get('original_query', query)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
        title_normalized = normalize_search_text(title)
        artist_normalized = normalize_search_text(artist)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ Ø¨Ø­Ø« Ø¥Ø¶Ø§ÙÙŠ
        combined_hash = hashlib.md5((title_normalized + artist_normalized + original_query).encode()).hexdigest()[:16]
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³Ø¬Ù„ Ø£ÙˆÙ„Ø§Ù‹
        cursor.execute("SELECT id FROM channel_index WHERE message_id = ? OR search_hash = ?", 
                      (message_id, search_hash))
        existing = cursor.fetchone()
        
        if existing:
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
            cursor.execute("""
                UPDATE channel_index 
                SET file_id = ?, file_unique_id = ?, title_normalized = ?, 
                    artist_normalized = ?, keywords_vector = ?, original_title = ?, 
                    original_artist = ?, duration = ?, file_size = ?, 
                    access_count = access_count + 1, popularity_rank = popularity_rank + 0.5,
                    last_accessed = CURRENT_TIMESTAMP
                WHERE message_id = ? OR search_hash = ?
            """, (
                file_id, file_unique_id, title_normalized, artist_normalized, 
                keywords_vector, title, artist, duration, file_size, 
                message_id, search_hash
            ))
            LOGGER(__name__).info(f"ğŸ”„ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        else:
            # Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
            cursor.execute("""
                INSERT INTO channel_index 
                (message_id, file_id, file_unique_id, search_hash, title_normalized, 
                 artist_normalized, keywords_vector, original_title, original_artist, 
                 duration, file_size, access_count, popularity_rank, phonetic_hash, partial_matches)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, 1.0, ?, ?)
            """, (
                message_id, file_id, file_unique_id, search_hash,
                title_normalized, artist_normalized, keywords_vector,
                title, artist, duration, file_size, combined_hash, original_query
            ))
            LOGGER(__name__).info(f"â• ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        conn.commit()
        conn.close()
        
        LOGGER(__name__).info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©: {title[:30]}")
        return True
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©: {e}")
        return False

async def send_cached_audio(event, status_msg, cache_result: Dict, bot_client):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ"""
    try:
        await status_msg.edit("ğŸ“¤ **Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ...**")
        
        original_message = cache_result['original_message']
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
        duration = cache_result.get('duration', 0)
        duration_str = f"{duration//60}:{duration%60:02d}" if duration > 0 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        user_caption = f"âœ¦ @{config.BOT_USERNAME}"
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
        await event.respond(
            user_caption,
            file=original_message.file,
            attributes=[
                DocumentAttributeAudio(
                    duration=duration,
                    title=cache_result.get('title', 'Unknown')[:60],
                    performer=cache_result.get('uploader', 'Unknown')[:40]
                )
            ]
        )
        
        await status_msg.delete()
        LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ")
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø®Ø²Ù†: {e}")
        await status_msg.edit("âŒ **Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ†**")

async def try_youtube_api_download(video_id: str, title: str) -> Optional[Dict]:
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube Data API"""
    try:
        import config
        import requests
        
        if not hasattr(config, 'YT_API_KEYS') or not config.YT_API_KEYS:
            LOGGER(__name__).warning("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙØ§ØªÙŠØ­ YouTube API")
            return None
        
        LOGGER(__name__).info("ğŸ”‘ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… YouTube Data API")
        
        for api_key in config.YT_API_KEYS:
            try:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
                api_url = f"https://www.googleapis.com/youtube/v3/videos"
                params = {
                    'part': 'snippet,contentDetails',
                    'id': video_id,
                    'key': api_key
                }
                
                response = requests.get(api_url, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if data.get('items'):
                        video_info = data['items'][0]
                        snippet = video_info['snippet']
                        
                        LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† API")
                        
                        # Ø§Ù„Ø¢Ù† Ù†Ø­Ø§ÙˆÙ„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª API
                        return await download_with_api_info(video_id, snippet, title)
                        
            except Exception as e:
                LOGGER(__name__).warning(f"âŒ ÙØ´Ù„ API key: {e}")
                continue
        
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ YouTube API: {e}")
        return None

async def download_with_api_info(video_id: str, snippet: dict, fallback_title: str) -> Optional[Dict]:
    """ØªØ­Ù…ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† YouTube API"""
    try:
        title = snippet.get('title', fallback_title)
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª API
        downloads_dir = Path("downloads")
        downloads_dir.mkdir(exist_ok=True)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù…Ù„Ù ÙƒÙˆÙƒÙŠØ² Ù…ØªØ§Ø­
        cookies_files = get_available_cookies()
        best_cookie = cookies_files[0] if cookies_files else None
        
        ydl_opts = {
            'format': 'bestaudio[filesize<30M]/best[filesize<30M]',  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø­Ø¬Ù…
            'outtmpl': str(downloads_dir / f'{video_id}_api.%(ext)s'),
            'quiet': True,
            'no_warnings': True,
            'noplaylist': True,
            'socket_timeout': 20,  # ÙˆÙ‚Øª Ù…Ø¹Ù‚ÙˆÙ„ Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
            'retries': 2,  # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…Ø¹Ù‚ÙˆÙ„Ø©
            'concurrent_fragment_downloads': 2,  # ØªØ­Ù…ÙŠÙ„ Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ØªØ¯Ù„
            'http_chunk_size': 5242880,  # 5MB chunks Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
            'prefer_ffmpeg': True,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… ffmpeg Ù„Ù„Ø³Ø±Ø¹Ø©
        }
        
        if best_cookie:
            ydl_opts['cookiefile'] = best_cookie
            LOGGER(__name__).info(f"ğŸª Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙˆÙƒÙŠØ²: {os.path.basename(best_cookie)}")
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(
                f"https://www.youtube.com/watch?v={video_id}",
                download=True
            )
            
            if info:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„
                for file_path in downloads_dir.glob(f"{video_id}_api.*"):
                    if file_path.suffix in ['.m4a', '.mp3', '.webm', '.mp4', '.opus']:
                        LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ø¨Ø± API")
                        return {
                            'success': True,
                            'file_path': str(file_path),
                            'title': title,
                            'duration': info.get('duration', 0),
                            'uploader': snippet.get('channelTitle', 'Unknown'),
                            'elapsed': 0
                        }
        
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ API: {e}")
        return None

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ
downloader = HyperSpeedDownloader()

async def simple_download(video_url: str, title: str) -> Optional[Dict]:
    """Ø¯Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙŠÙ„Ø© Ø¨Ø³ÙŠØ·Ø©"""
    try:
        LOGGER(__name__).info(f"ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙŠÙ„: {video_url}")
        
        downloads_dir = Path("downloads")
        downloads_dir.mkdir(exist_ok=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ video_id Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
        video_id = video_url.split('=')[-1] if '=' in video_url else 'unknown'
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© 1: ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… youtube-dl Ø¨Ø³ÙŠØ·
        try:
            import subprocess
            import json
            
            LOGGER(__name__).info("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© youtube-dl Ù…Ø¨Ø§Ø´Ø±")
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… youtube-dl Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±
            cmd = ['youtube-dl', '-j', '--no-playlist', f'https://www.youtube.com/watch?v={video_id}']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                video_data = json.loads(result.stdout)
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· ØµÙˆØªÙŠ Ù…Ø¨Ø§Ø´Ø±
                formats = video_data.get('formats', [])
                audio_formats = [f for f in formats if f.get('acodec') != 'none' and f.get('vcodec') == 'none']
                
                if audio_formats:
                    # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø© ØµÙˆØªÙŠØ©
                    best_audio = sorted(audio_formats, key=lambda x: x.get('abr', 0), reverse=True)[0]
                    audio_url = best_audio['url']
                    
                    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
                    import requests
                    response = requests.get(audio_url, timeout=60, stream=True)
                    
                    if response.status_code == 200:
                        file_path = downloads_dir / f"{video_id}_direct.{best_audio.get('ext', 'm4a')}"
                        
                        with open(file_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        
                        if file_path.exists() and file_path.stat().st_size > 1000:  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 1KB
                            LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¨Ù†Ø¬Ø§Ø­")
                            return {
                                'audio_path': str(file_path),
                                'title': video_data.get('title', title),
                                'duration': video_data.get('duration', 0),
                                'artist': video_data.get('uploader', 'Unknown'),
                                'source': 'Direct Download'
                            }
                            
        except Exception as e:
            LOGGER(__name__).warning(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: {e}")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… invidious ÙƒØ¨Ø¯ÙŠÙ„
        try:
            import requests
            
            # Ù‚Ø§Ø¦Ù…Ø© Ø®ÙˆØ§Ø¯Ù… invidious
            invidious_instances = [
                'https://invidious.io',
                'https://invidious.snopyta.org',
                'https://yewtu.be',
                'https://invidious.kavin.rocks'
            ]
            
            for instance in invidious_instances:
                try:
                    LOGGER(__name__).info(f"ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© {instance}")
                    
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
                    api_url = f"{instance}/api/v1/videos/{video_id}"
                    response = requests.get(api_url, timeout=10)
                    
                    if response.status_code == 200:
                        video_data = response.json()
                        
                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· ØµÙˆØªÙŠ
                        audio_formats = [f for f in video_data.get('adaptiveFormats', []) if 'audio' in f.get('type', '')]
                        
                        if audio_formats:
                            audio_url = audio_formats[0]['url']
                            
                            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
                            audio_response = requests.get(audio_url, timeout=30, stream=True)
                            
                            if audio_response.status_code == 200:
                                file_path = downloads_dir / f"{video_id}_invidious.m4a"
                                
                                with open(file_path, 'wb') as f:
                                    for chunk in audio_response.iter_content(chunk_size=8192):
                                        f.write(chunk)
                                
                                if file_path.exists():
                                    LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† {instance}")
                                    return {
                                        'audio_path': str(file_path),
                                        'title': video_data.get('title', title),
                                        'duration': video_data.get('lengthSeconds', 0),
                                        'artist': video_data.get('author', 'Unknown'),
                                        'source': 'Invidious'
                                    }
                        break
                        
                except Exception as e:
                    LOGGER(__name__).warning(f"âŒ ÙØ´Ù„ {instance}: {e}")
                    continue
                    
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Invidious: {e}")
        
        # Ø¥Ø°Ø§ ÙØ´Ù„ ÙƒÙ„ Ø´ÙŠØ¡ØŒ Ù„Ø§ Ù†Ù†Ø´Ø¦ Ù…Ù„Ù TXT
        LOGGER(__name__).error("âŒ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©")
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø¯ÙŠÙ„: {e}")
        return None

async def send_audio_file(event, status_msg, audio_file: str, result: dict, query: str = "", bot_client=None):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ­ÙØ¸Ù‡ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ"""
    try:
        await status_msg.edit("ğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù...**")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©
        duration = result.get('duration', 0)
        duration_str = f"{duration//60}:{duration%60:02d}" if duration > 0 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        caption = f"âœ¦ @{config.BOT_USERNAME}"
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
        thumb_path = None
        try:
            if 'thumbnail' in result and result['thumbnail']:
                thumb_path = await download_thumbnail(
                    result['thumbnail'], 
                    result.get('title', 'Unknown'), 
                    result.get('id', None)
                )
        except Exception as thumb_error:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {thumb_error}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
        await event.respond(
            caption,
            file=audio_file,
            thumb=thumb_path,
            attributes=[
                DocumentAttributeAudio(
                    duration=duration,
                    title=result.get('title', 'Unknown')[:60],
                    performer=result.get('uploader', 'Unknown')[:40]
                )
            ]
        )
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ (ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©)
        if query and bot_client:
            try:
                LOGGER(__name__).info(f"ğŸ’¾ Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...")
                saved = await save_to_smart_cache(bot_client, audio_file, result, query, thumb_path)
                if saved:
                    LOGGER(__name__).info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ")
                else:
                    LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ")
            except Exception as cache_error:
                LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ: {cache_error}")
        
        await status_msg.delete()
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        await remove_temp_files(audio_file)
        
        # Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
        if thumb_path and os.path.exists(thumb_path):
            try:
                os.remove(thumb_path)
                LOGGER(__name__).debug(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {os.path.basename(thumb_path)}")
            except Exception as e:
                LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {e}")
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù: {e}")

async def try_alternative_downloads(video_id: str, title: str) -> Optional[Dict]:
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø·Ø±Ù‚ ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙŠÙ„Ø©"""
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© 1: YouTube API
        api_result = await try_youtube_api_download(video_id, title)
        if api_result and api_result.get('success'):
            return api_result
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© 2: ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©)
        cookies_files = get_available_cookies()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        distribution = calculate_cookies_distribution(len(cookies_files))
        primary_count = distribution['primary']
        secondary_count = distribution['secondary']
        
        if secondary_count == 0:
            LOGGER(__name__).info("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙˆÙƒÙŠØ² Ø«Ø§Ù†ÙˆÙŠØ© - ØªØ®Ø·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©")
            return None
        
        start_index = primary_count
        end_index = primary_count + secondary_count
        
        LOGGER(__name__).info(f"ğŸ”„ Ø§Ø³ØªØ®Ø¯Ø§Ù… {secondary_count} ÙƒÙˆÙƒÙŠØ² Ø«Ø§Ù†ÙˆÙŠ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø± {start_index} Ø¥Ù„Ù‰ {end_index}")
        
        for i, cookie_file in enumerate(cookies_files[start_index:end_index], start_index + 1):
            try:
                LOGGER(__name__).info(f"ğŸª Ù…Ø­Ø§ÙˆÙ„Ø© ÙƒÙˆÙƒÙŠØ² Ø¨Ø¯ÙŠÙ„ #{i}: {os.path.basename(cookie_file)}")
                
                downloads_dir = Path("downloads")
                ydl_opts = {
                    'format': 'bestaudio[filesize<25M]/best[filesize<25M]',
                    'outtmpl': str(downloads_dir / f'{video_id}_alt_{i}.%(ext)s'),
                    'quiet': True,
                    'no_warnings': True,
                    'noplaylist': True,
                    'cookiefile': cookie_file,
                    'socket_timeout': 18,  # ÙˆÙ‚Øª Ù…Ø¹Ù‚ÙˆÙ„ Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
                    'retries': 2,
                    'concurrent_fragment_downloads': 2,
                    'http_chunk_size': 4194304,  # 4MB chunks
                }
                
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(
                        f"https://www.youtube.com/watch?v={video_id}",
                        download=True
                    )
                    
                    if info:
                        # ØªØªØ¨Ø¹ Ù†Ø¬Ø§Ø­ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                        track_cookie_usage(cookie_file, success=True)
                        
                        for file_path in downloads_dir.glob(f"{video_id}_alt_{i}.*"):
                            if file_path.suffix in ['.m4a', '.mp3', '.webm', '.mp4', '.opus']:
                                return {
                                    'success': True,
                                    'file_path': str(file_path),
                                    'title': info.get('title', title),
                                    'duration': info.get('duration', 0),
                                    'uploader': info.get('uploader', 'Unknown'),
                                    'elapsed': 0
                                }
                                
            except Exception as e:
                error_msg = str(e).lower()
                LOGGER(__name__).warning(f"âŒ ÙØ´Ù„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ø¨Ø¯ÙŠÙ„ #{i}: {e}")
                
                # ØªØªØ¨Ø¹ ÙØ´Ù„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙˆØ­Ø¸Ø± Ø§Ù„Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡Ø§
                track_cookie_usage(cookie_file, success=False)
                
                if any(keyword in error_msg for keyword in [
                    'blocked', 'forbidden', '403', 'unavailable', 'cookies', 'expired',
                    'sign in', 'login', 'authentication', 'token', 'session', 'captcha'
                ]):
                    mark_cookie_as_blocked(cookie_file, f"Ø¨Ø¯ÙŠÙ„: {str(e)[:50]}")
                
                continue
        
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©: {e}")
        return None

async def force_download_any_way(video_id: str, title: str) -> Optional[Dict]:
    """Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù‚Ø³Ø±ÙŠ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© (Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©)
        cookies_files = get_available_cookies()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        distribution = calculate_cookies_distribution(len(cookies_files))
        primary_count = distribution['primary']
        secondary_count = distribution['secondary']
        remaining_count = distribution['remaining']
        
        if remaining_count == 0:
            LOGGER(__name__).info("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙˆÙƒÙŠØ² Ù…ØªØ¨Ù‚ÙŠØ© Ù„Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù‚Ø³Ø±ÙŠØ©")
            return None
        
        start_index = primary_count + secondary_count
        end_index = start_index + remaining_count
        remaining_files = cookies_files[start_index:end_index]
        
        LOGGER(__name__).info(f"ğŸš€ Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø³Ø±ÙŠØ© Ù…Ø¹ {len(remaining_files)} Ù…Ù„Ù ÙƒÙˆÙƒÙŠØ² Ù…ØªØ¨Ù‚ÙŠ (Ù…Ù† {start_index} Ø¥Ù„Ù‰ {end_index})")
        
        for i, cookie_file in enumerate(remaining_files, start_index + 1):
            try:
                LOGGER(__name__).info(f"ğŸš€ Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø³Ø±ÙŠØ© #{i}: {os.path.basename(cookie_file)}")
                
                downloads_dir = Path("downloads")
                ydl_opts = {
                    'format': 'bestaudio[filesize<25M]/best[filesize<25M]',  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø­Ø¬Ù…
                    'outtmpl': str(downloads_dir / f'{video_id}_force_{i}.%(ext)s'),
                    'quiet': True,
                    'no_warnings': True,
                    'noplaylist': True,
                    'cookiefile': cookie_file,
                    'socket_timeout': 18,  # ÙˆÙ‚Øª Ù…Ø¹Ù‚ÙˆÙ„
                    'retries': 2,  # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…Ø¹Ù‚ÙˆÙ„Ø©
                    'ignore_errors': True,
                    'concurrent_fragment_downloads': 2,
                    'http_chunk_size': 4194304,  # 4MB chunks
                }
                
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(
                        f"https://www.youtube.com/watch?v={video_id}",
                        download=True
                    )
                    
                    if info:
                        # ØªØªØ¨Ø¹ Ù†Ø¬Ø§Ø­ Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù‚Ø³Ø±ÙŠØ©
                        track_cookie_usage(cookie_file, success=True)
                        
                        for file_path in downloads_dir.glob(f"{video_id}_force_{i}.*"):
                            if file_path.exists() and file_path.stat().st_size > 1000:
                                LOGGER(__name__).info(f"ğŸ‰ Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø³Ø±ÙŠ Ø¨Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {os.path.basename(cookie_file)}")
                                return {
                                    'success': True,
                                    'file_path': str(file_path),
                                    'title': info.get('title', title),
                                    'duration': info.get('duration', 0),
                                    'uploader': info.get('uploader', 'Unknown'),
                                    'elapsed': 0
                                }
                                
            except Exception as e:
                error_msg = str(e).lower()
                LOGGER(__name__).warning(f"âŒ ÙØ´Ù„ Ø§Ù„Ù‚Ø³Ø±ÙŠ #{i}: {e}")
                
                # ØªØªØ¨Ø¹ ÙØ´Ù„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙˆØ­Ø¸Ø± Ø§Ù„ØªØ§Ù„ÙØ© Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹
                track_cookie_usage(cookie_file, success=False)
                
                if any(keyword in error_msg for keyword in [
                    'blocked', 'forbidden', '403', 'unavailable', 'cookies', 'expired',
                    'sign in', 'login', 'authentication', 'token', 'session', 'captcha',
                    'invalid', 'corrupt'
                ]):
                    mark_cookie_as_blocked(cookie_file, f"Ù‚Ø³Ø±ÙŠ: {str(e)[:50]}")
                    LOGGER(__name__).error(f"ğŸ’€ ÙƒÙˆÙƒÙŠØ² ØªØ§Ù„Ù Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹: {os.path.basename(cookie_file)}")
                
                continue
        
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø³Ø±ÙŠ: {e}")
        return None

async def remove_temp_files(*paths):
    """Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†"""
    for path in paths:
        if path and os.path.exists(path):
            try:
                os.remove(path)
                LOGGER(__name__).debug(f"ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª: {path}")
            except Exception as e:
                LOGGER(__name__).warning(f"ÙØ´Ù„ Ø­Ø°Ù {path}: {e}")

async def download_thumbnail(url: str, title: str, video_id: str = None) -> Optional[str]:
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø© Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
    if not url:
        return None
    
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… video_id Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹ØŒ ÙˆØ¥Ù„Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù†Ø¸Ù
        if video_id:
            thumb_path = f"downloads/thumb_{video_id}.jpg"
        else:
            title_clean = re.sub(r'[\\/*?:"<>|]', "", title)
            thumb_path = f"downloads/thumb_{title_clean[:20]}.jpg"
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        if os.path.exists(thumb_path):
            return thumb_path
        
        LOGGER(__name__).info(f"ğŸ“¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {os.path.basename(thumb_path)}")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status == 200:
                    async with aiofiles.open(thumb_path, mode='wb') as f:
                        await f.write(await resp.read())
                    LOGGER(__name__).info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {os.path.basename(thumb_path)}")
                    return thumb_path
                else:
                    LOGGER(__name__).warning(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: HTTP {resp.status}")
    except Exception as e:
        LOGGER(__name__).warning(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {e}")
    
    return None

async def process_unlimited_download_enhanced(event, user_id: int, start_time: float):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    task_id = f"{user_id}_{int(time.time() * 1000000)}"  # Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
    
    try:
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙˆØ±Ø§Ù‹ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        active_downloads[task_id] = {
            'user_id': user_id,
            'start_time': start_time,
            'task_id': task_id,
            'status': 'started_enhanced',
            'phase': 'initialization'
        }
        
        LOGGER(__name__).info(f"ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} | Ø§Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ Ù…Ù‡Ù…Ø© Ù…Ù†ÙØµÙ„Ø© - Ø¨Ø¯ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ø±
        asyncio.create_task(execute_parallel_download_enhanced(event, user_id, start_time, task_id))
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
        await update_performance_stats(False, time.time() - start_time)
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù‡Ù…Ø©
        if task_id in active_downloads:
            del active_downloads[task_id]
            LOGGER(__name__).info(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©: {task_id} - Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {len(active_downloads)}")

async def execute_parallel_download_enhanced(event, user_id: int, start_time: float, task_id: str):
    """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©"""
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        match = event.pattern_match
        if not match:
            await event.reply("âŒ **Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨**")
            return
        
        query = match.group(2) if match.group(2) else ""
        if not query:
            await event.reply("ğŸ“ **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** `Ø¨Ø­Ø« Ø§Ø³Ù… Ø§Ù„Ø£ØºÙ†ÙŠØ©`")
            await update_performance_stats(False, time.time() - start_time)
            return
        
        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
        if task_id in active_downloads:
            active_downloads[task_id].update({
                'query': query,
                'status': 'processing_enhanced',
                'phase': 'search_preparation'
            })
        
        LOGGER(__name__).info(f"ğŸµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…Ø­Ø³Ù†Ø©: {query} | Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_id} | Ø§Ù„Ù…Ù‡Ù…Ø©: {task_id}")
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø­Ù„Ø©
        if task_id in active_downloads:
            active_downloads[task_id]['phase'] = 'intelligent_search'
        
        # Ù…ØªØºÙŠØ± Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø© (Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©)
        status_msg = None
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø¨ØµÙ…Øª - Ø¨Ø¯ÙˆÙ† Ø±Ø³Ø§Ø¦Ù„ Ù…Ø²Ø¹Ø¬Ø©
        # status_msg Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙÙ‚Ø·
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯
        try:
            parallel_result = await parallel_search_with_monitoring(query, event.client)
            
            if parallel_result and parallel_result.get('success'):
                search_source = parallel_result.get('search_source', 'unknown')
                search_time = parallel_result.get('search_time', 0)
                processed_msgs = parallel_result.get('processed_messages', 0)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                await update_performance_stats(True, time.time() - start_time, from_cache=True)
                
                if search_source == 'database':
                    if not status_msg:
                        status_msg = await event.reply(f"âœ… **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ ({search_time:.2f}s)**\n\nğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                    else:
                        await status_msg.edit(f"âœ… **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ ({search_time:.2f}s)**\n\nğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                    success = await send_cached_from_database(event, status_msg, parallel_result, event.client)
                    if success:
                        return  # Ù†Ø¬Ø­ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
                    else:
                        LOGGER(__name__).warning("âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ - Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† ÙŠÙˆØªÙŠÙˆØ¨")
                elif search_source == 'smart_cache':
                    if not status_msg:
                        status_msg = await event.reply(f"âœ… **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ ({search_time:.2f}s)**\n\nğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                    else:
                        await status_msg.edit(f"âœ… **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ ({search_time:.2f}s)**\n\nğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                    success = await send_cached_from_telegram(event, status_msg, parallel_result, event.client)
                    if success:
                        return  # Ù†Ø¬Ø­ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ
                    else:
                        LOGGER(__name__).warning("âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ - Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† ÙŠÙˆØªÙŠÙˆØ¨")
            else:
                # Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ - Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙˆÙ† Ø¥Ø²Ø¹Ø§Ø¬
                pass
                
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ: {e}")
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© - Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„
            
        # Ø§Ù„Ø¨Ø¯ÙŠÙ„: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø·ÙˆØ±
        try:
            LOGGER(__name__).info(f"ğŸ”„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø·ÙˆØ± ÙƒØ¨Ø¯ÙŠÙ„: {query}")
            await download_song_smart(event, query)
            return
        except Exception as e:
            LOGGER(__name__).error(f"âŒ ÙØ´Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø·ÙˆØ±: {e}")
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ†ØŒ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ
        if not status_msg:
            status_msg = await event.reply("ğŸ” **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† ÙŠÙˆØªÙŠÙˆØ¨...**")
        else:
            await status_msg.edit("ğŸ” **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† ÙŠÙˆØªÙŠÙˆØ¨...**")
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø­Ù„Ø©
        if task_id in active_downloads:
            active_downloads[task_id]['phase'] = 'youtube_download'
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù†
        await process_smart_youtube_download(event, status_msg, query, user_id, start_time, task_id)
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©: {e}")
        await update_performance_stats(False, time.time() - start_time)
        
        try:
            await event.reply("âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ Ø§Ù„Ù…Ø­Ø³Ù†**")
        except:
            pass

async def process_smart_youtube_download(event, status_msg, query: str, user_id: int, start_time: float, task_id: str):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù…Ù† ÙŠÙˆØªÙŠÙˆØ¨ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    try:
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø­Ù„Ø©
        if task_id in active_downloads:
            active_downloads[task_id]['phase'] = 'youtube_search'
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙÙŠ ÙŠÙˆØªÙŠÙˆØ¨
        await status_msg.edit("ğŸ” **Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙÙŠ ÙŠÙˆØªÙŠÙˆØ¨...**")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· Ø£ÙˆÙ„Ø§Ù‹ (API + yt-dlp)
        try:
            from ZeMusic.plugins.play.youtube_api_downloader import search_and_download_hybrid
            hybrid_result = await search_and_download_hybrid(query)
            
            if hybrid_result and hybrid_result.get('success'):
                LOGGER(__name__).info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„Ø·: {hybrid_result['title']}")
                result = {
                    'audio_path': hybrid_result['file_path'],
                    'title': hybrid_result['title'],
                    'duration': hybrid_result['duration'],
                    'uploader': hybrid_result['uploader'],
                    'video_id': hybrid_result['video_id'],
                    'method': 'hybrid_api_ytdlp'
                }
            else:
                LOGGER(__name__).info("âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø®ØªÙ„Ø·ØŒ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ")
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª
                result = await downloader.hyper_download(query)
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø·: {e}")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª
            result = await downloader.hyper_download(query)
        
        if result:
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø­Ù„Ø©
            if task_id in active_downloads:
                active_downloads[task_id]['phase'] = 'sending_file'
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ
            await send_audio_file(event, status_msg, result['audio_path'], result, query, event.client)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            await update_performance_stats(True, time.time() - start_time)
            
            LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†: {query}")
        else:
            await status_msg.edit("âŒ **Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£ØºÙ†ÙŠØ©**\n\nğŸ’¡ **Ø¬Ø±Ø¨:**\nâ€¢ ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©\nâ€¢ Ø§Ø³Ù… Ø§Ù„ÙÙ†Ø§Ù†\nâ€¢ Ø¬Ø²Ø¡ Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØºÙ†ÙŠØ©")
            await update_performance_stats(False, time.time() - start_time)
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ: {e}")
        await status_msg.edit("âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„**")
        await update_performance_stats(False, time.time() - start_time)

# --- Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…Ø·ÙˆØ± Ù…Ø¹ Telethon ---
async def cache_stats_handler(event):
    """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        async with downloader.conn_manager.db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM channel_index")
            total_cached = cursor.fetchone()[0]
            
            cursor.execute("SELECT SUM(access_count) FROM channel_index")
            total_hits = cursor.fetchone()[0] or 0
            
            cursor.execute("SELECT original_title, access_count FROM channel_index ORDER BY access_count DESC LIMIT 5")
            top_songs = cursor.fetchall()
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
            mem_usage = psutil.virtual_memory().percent
            cpu_usage = psutil.cpu_percent()
            active_tasks = len(downloader.active_tasks)
            cache_hit_rate = downloader.cache_hits / max(1, downloader.cache_hits + downloader.cache_misses) * 100
            
            stats_text = f"""ğŸ“Š **Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©**

ğŸ’¾ **Ø§Ù„Ù…Ø­ÙÙˆØ¸:** {total_cached} Ù…Ù„Ù
âš¡ **Ù…Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** {total_hits}
ğŸ“ˆ **Ù†Ø³Ø¨Ø© Ø§Ù„ÙƒØ§Ø´:** {cache_hit_rate:.1f}%
ğŸ“º **Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†:** {SMART_CACHE_CHANNEL or "ØºÙŠØ± Ù…ÙØ¹Ø¯Ø©"}

ğŸ§  **Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:**
â€¢ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {mem_usage}%
â€¢ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {cpu_usage}%
â€¢ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù†Ø´Ø·Ø©: {active_tasks}

ğŸµ **Ø§Ù„Ø£ÙƒØ«Ø± Ø·Ù„Ø¨Ø§Ù‹:**"""
            
            for i, row in enumerate(top_songs, 1):
                stats_text += f"\n{i}. {row[0][:30]}... ({row[1]})"
            
            await event.reply(stats_text)
            
    except Exception as e:
        await event.reply(f"âŒ Ø®Ø·Ø£: {e}")

async def clear_cache_handler(event):
    """Ù…Ø³Ø­ ÙƒØ§Ø´ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        async with downloader.conn_manager.db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM channel_index")
            total_before = cursor.fetchone()[0]
            cursor.execute("DELETE FROM channel_index")
            conn.commit()
        
        downloader.cache_hits = 0
        downloader.cache_misses = 0
        
        await event.reply(f"""ğŸ§¹ **ØªÙ… Ù…Ø³Ø­ ÙƒØ§Ø´ Ø§Ù„ØªØ®Ø²ÙŠÙ†!**

ğŸ“Š **Ø§Ù„Ù…Ø­Ø°ÙˆÙ:** {total_before} Ù…Ù„Ù
ğŸ’½ **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** ØªÙ… ØªÙ†Ø¸ÙŠÙÙ‡Ø§
ğŸ”„ **Ø§Ù„ÙƒØ§Ø´:** ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†Ù‡

âš¡ Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙƒØ§Ø´ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…""")
        
    except Exception as e:
        await event.reply(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´: {e}")

async def system_stats_handler(event):
    """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        mem = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        load_avg = os.getloadavg()
        
        stats_text = f"""ğŸ“¡ **Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©**

ğŸ§  **Ø§Ù„Ø°Ø§ÙƒØ±Ø©:**
â€¢ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {mem.total // (1024**3)} GB
â€¢ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {mem.used // (1024**3)} GB
â€¢ Ø§Ù„Ø­Ø±: {mem.free // (1024**3)} GB
â€¢ Ø§Ù„Ù†Ø³Ø¨Ø©: {mem.percent}%

ğŸ’¾ **Ø§Ù„ØªØ®Ø²ÙŠÙ†:**
â€¢ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {disk.total // (1024**3)} GB
â€¢ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {disk.used // (1024**3)} GB
â€¢ Ø§Ù„Ù†Ø³Ø¨Ø©: {disk.percent}%

âš™ï¸ **Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬:**
â€¢ Ø§Ù„Ù†ÙˆÙ‰: {psutil.cpu_count()}
â€¢ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {psutil.cpu_percent()}%
â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ (1/5/15 Ø¯): {load_avg[0]:.2f}/{load_avg[1]:.2f}/{load_avg[2]:.2f}

ğŸ“¶ **Ø§Ù„Ø´Ø¨ÙƒØ©:**
â€¢ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {len(psutil.net_connections())}
"""
        await event.reply(stats_text)
        
    except Exception as e:
        await event.reply(f"âŒ Ø®Ø·Ø£: {e}")

# --- Ø¥Ø¯Ø§Ø±Ø© Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù… ---
async def shutdown_system():
    """Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†"""
    try:
        LOGGER(__name__).info("ğŸ”´ Ø¨Ø¯Ø¡ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù…...")
        if hasattr(downloader, 'conn_manager') and downloader.conn_manager:
            await downloader.conn_manager.close()
        LOGGER(__name__).info("âœ… ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
    except Exception as e:
        LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")

# ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚
atexit.register(lambda: asyncio.run(shutdown_system()))

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ù…Ø·ÙˆØ±
async def download_song_smart(message, query: str):
    """
    Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
    
    Ø§Ù„Ù…Ø±Ø§Ø­Ù„:
    1. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ÙˆÙ‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
    2. Ø¥Ø±Ø³Ø§Ù„ ÙÙˆØ±ÙŠ Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯ Ø§Ù„Ù…Ù‚Ø·Ø¹
    3. Ø§Ù†ØªÙ‚Ø§Ù„ Ù…ØªØ³Ù„Ø³Ù„ Ù„Ù„Ø·Ø±Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯
    """
    try:
        # Ù…ØªØºÙŠØ± Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø© (Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©)
        status_msg = None
        
        LOGGER(__name__).info(f"ğŸµ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}")
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ÙˆÙ‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
        cache_result, telegram_result = await parallel_cache_search(query, message.client)
        
        # ÙØ­Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        if cache_result:
            LOGGER(__name__).info("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ")
            if not status_msg:
                status_msg = await message.reply("ğŸ“ **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´!**\nğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...")
            else:
                await status_msg.edit("ğŸ“ **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´!**\nğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...")
            
            success = await send_local_cached_audio(message, cache_result, status_msg)
            if success:
                return
                
        elif telegram_result:
            LOGGER(__name__).info("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†")
            if not status_msg:
                status_msg = await message.reply("ğŸ“º **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†!**\nğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...")
            else:
                await status_msg.edit("ğŸ“º **ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†!**\nğŸ“¤ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...")
            
            success = await send_telegram_cached_audio(message, telegram_result, status_msg)
            if success:
                return
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø·Ø¹ - Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ
        LOGGER(__name__).info("ğŸ” Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ - Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ")
        if not status_msg:
            status_msg = await message.reply("ğŸ” **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ YouTube...**")
        else:
            await status_msg.edit("ğŸ” **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ YouTube...**")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„ ÙÙŠ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
        video_info = await sequential_external_search(query)
        
        if not video_info:
            if not status_msg:
                status_msg = await message.reply(
                    "âŒ **Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬**\n\n"
                    "ğŸ’¡ **Ø¬Ø±Ø¨:**\n"
                    "â€¢ ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©\n"
                    "â€¢ Ø§Ø³Ù… Ø§Ù„ÙÙ†Ø§Ù†\n"
                    "â€¢ Ø¬Ø²Ø¡ Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØºÙ†ÙŠØ©"
                )
            else:
                await status_msg.edit(
                    "âŒ **Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬**\n\n"
                    "ğŸ’¡ **Ø¬Ø±Ø¨:**\n"
                    "â€¢ ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©\n"
                    "â€¢ Ø§Ø³Ù… Ø§Ù„ÙÙ†Ø§Ù†\n"
                    "â€¢ Ø¬Ø²Ø¡ Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØºÙ†ÙŠØ©"
                )
            return
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ cookies
        LOGGER(__name__).info(f"â¬‡ï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ: {video_info.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        success = await smart_download_and_send(message, video_info, status_msg)
        
        if not success:
            if not status_msg:
                status_msg = await message.reply(
                    "âŒ **ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„**\n\n"
                    "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø·Ø¹\n"
                    "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹"
                )
            else:
                await status_msg.edit(
                    "âŒ **ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„**\n\n"
                    "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø·Ø¹\n"
                    "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹"
                )
        
    except Exception as e:
        LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ download_song_smart: {e}")
        try:
            await message.reply(
                "âŒ **Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«**\n\n"
                "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ\n"
                "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰"
            )
        except:
            pass

# === Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ù…Ø·ÙˆØ± ===

async def parallel_cache_search(query: str, bot_client) -> Tuple[Optional[Dict], Optional[Dict]]:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆÙ‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø¯Ù‚ÙŠÙ‚Ø©"""
    start_time = time.time()
    
    try:
        LOGGER(__name__).info(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ: {query}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        if not query or not query.strip():
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« ÙØ§Ø±Øº")
            return None, None
            
        if not bot_client:
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙˆØª ØºÙŠØ± Ù…ØªØ§Ø­")
            return None, None
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        cleaned_query = query.strip()
        LOGGER(__name__).debug(f"ğŸ§¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ù†Ø¸Ù: {cleaned_query}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ ÙØ±Ø¯ÙŠØ©
        cache_task = None
        telegram_task = None
        
        try:
            cache_task = asyncio.create_task(search_local_cache(cleaned_query))
            LOGGER(__name__).debug("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ")
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ: {e}")
            
        try:
            telegram_task = asyncio.create_task(search_in_telegram_cache(cleaned_query, bot_client))
            LOGGER(__name__).debug("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…")
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
        if not cache_task and not telegram_task:
            LOGGER(__name__).error("âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙŠ Ù…Ù† Ù…Ù‡Ø§Ù… Ø§Ù„Ø¨Ø­Ø«")
            return None, None
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ timeout ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù…Ø©
        cache_result = None
        telegram_result = None
        
        try:
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·
            tasks = []
            if cache_task:
                tasks.append(cache_task)
            if telegram_task:
                tasks.append(telegram_task)
            
            LOGGER(__name__).info(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {len(tasks)} Ù…Ù‡Ù…Ø© Ø¨Ø­Ø« Ù…Ø¹ Ù…Ù‡Ù„Ø© 10 Ø«ÙˆØ§Ù†...")
            
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=10.0
            )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            result_index = 0
            if cache_task:
                cache_result = results[result_index]
                result_index += 1
                
                if isinstance(cache_result, Exception):
                    LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ: {cache_result}")
                    cache_result = None
                elif cache_result:
                    LOGGER(__name__).info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ: {cache_result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                else:
                    LOGGER(__name__).debug("ğŸ” Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ")
                    
            if telegram_task:
                telegram_result = results[result_index]
                
                if isinstance(telegram_result, Exception):
                    LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…: {telegram_result}")
                    telegram_result = None
                elif telegram_result:
                    LOGGER(__name__).info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…: {telegram_result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                else:
                    LOGGER(__name__).debug("ğŸ” Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…")
            
        except asyncio.TimeoutError:
            LOGGER(__name__).warning("â° Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ (10 Ø«ÙˆØ§Ù†)")
            
            # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
            if cache_task and not cache_task.done():
                cache_task.cancel()
                LOGGER(__name__).debug("ğŸš« ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ")
                
            if telegram_task and not telegram_task.done():
                telegram_task.cancel()
                LOGGER(__name__).debug("ğŸš« ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…")
                
        except Exception as gather_error:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«: {gather_error}")
            import traceback
            LOGGER(__name__).error(f"ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£: {traceback.format_exc()}")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        elapsed_time = time.time() - start_time
        LOGGER(__name__).info(
            f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ:\n"
            f"   â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {elapsed_time:.2f} Ø«Ø§Ù†ÙŠØ©\n"
            f"   ğŸ“ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ: {'âœ… Ù†Ø¬Ø­' if cache_result else 'âŒ ÙØ´Ù„/ÙØ§Ø±Øº'}\n"
            f"   ğŸ“º Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…: {'âœ… Ù†Ø¬Ø­' if telegram_result else 'âŒ ÙØ´Ù„/ÙØ§Ø±Øº'}"
        )
        
        return cache_result, telegram_result
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        LOGGER(__name__).error(
            f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ:\n"
            f"   ğŸ” Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}\n"
            f"   â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {elapsed_time:.2f} Ø«Ø§Ù†ÙŠØ©\n"
            f"   ğŸ“‹ Ø§Ù„Ø®Ø·Ø£: {str(e)}"
        )
        import traceback
        LOGGER(__name__).error(f"ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ÙƒØ§Ù…Ù„Ø©: {traceback.format_exc()}")
        return None, None

async def search_local_cache(query: str) -> Optional[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ (Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª) Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø¯Ù‚ÙŠÙ‚Ø©"""
    start_time = time.time()
    conn = None
    
    try:
        LOGGER(__name__).info(f"ğŸ“ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ: {query}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        if not query or not query.strip():
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« ÙØ§Ø±Øº ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ")
            return None
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù„Ù„Ø¨Ø­Ø«
        try:
            normalized_query = normalize_arabic_text(query)
            LOGGER(__name__).debug(f"ğŸ§¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ù†Ø¸Ù: '{normalized_query}'")
            
            if not normalized_query:
                LOGGER(__name__).warning("âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ù†Ø¸Ù ÙØ§Ø±Øº")
                return None
                
            search_keywords = normalized_query.split()
            LOGGER(__name__).debug(f"ğŸ”‘ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«: {search_keywords}")
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
            return None
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if not os.path.exists(DATABASE_PATH):
            LOGGER(__name__).warning(f"âš ï¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {DATABASE_PATH}")
            return None
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        try:
            conn = sqlite3.connect(DATABASE_PATH, timeout=5.0)
            cursor = conn.cursor()
            LOGGER(__name__).debug("âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
        except sqlite3.Error as db_error:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {db_error}")
            return None
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„
        try:
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='cached_audio'")
            table_exists = cursor.fetchone()
            
            if not table_exists:
                LOGGER(__name__).warning("âš ï¸ Ø¬Ø¯ÙˆÙ„ cached_audio ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                return None
                
            LOGGER(__name__).debug("âœ… Ø¬Ø¯ÙˆÙ„ cached_audio Ù…ÙˆØ¬ÙˆØ¯")
            
        except sqlite3.Error as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}")
            return None
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        search_conditions = []
        search_params = []
        
        try:
            for keyword in search_keywords:
                if keyword.strip():  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©
                    search_conditions.append(
                        "(LOWER(title) LIKE ? OR LOWER(artist) LIKE ? OR LOWER(keywords) LIKE ?)"
                    )
                    keyword_lower = keyword.lower()
                    search_params.extend([f"%{keyword_lower}%", f"%{keyword_lower}%", f"%{keyword_lower}%"])
            
            if not search_conditions:
                LOGGER(__name__).warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø´Ø±ÙˆØ· Ø¨Ø­Ø« ØµØ§Ù„Ø­Ø©")
                return None
                
            where_clause = " AND ".join(search_conditions)
            query_sql = f"""
            SELECT video_id, title, artist, duration, file_path, thumb, message_id, keywords, created_at
            FROM cached_audio 
            WHERE {where_clause}
            ORDER BY created_at DESC LIMIT 1
            """
            
            LOGGER(__name__).debug(f"ğŸ“‹ Ø§Ø³ØªØ¹Ù„Ø§Ù… SQL: {query_sql}")
            LOGGER(__name__).debug(f"ğŸ“‹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¨Ø­Ø«: {len(search_params)} Ù…Ø¹Ø§Ù…Ù„")
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«: {e}")
            return None
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        try:
            cursor.execute(query_sql, search_params)
            result = cursor.fetchone()
            
            if result:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©
                try:
                    result_dict = {
                        "video_id": result[0] if result[0] else "unknown",
                        "title": result[1] if result[1] else "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        "artist": result[2] if result[2] else "ÙÙ†Ø§Ù† ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        "duration": int(result[3]) if result[3] and str(result[3]).isdigit() else 0,
                        "file_path": result[4] if result[4] else None,
                        "thumb": result[5] if result[5] else None,
                        "message_id": int(result[6]) if result[6] and str(result[6]).isdigit() else None,
                        "keywords": result[7] if result[7] else "",
                        "source": "local_cache",
                        "created_at": result[8] if result[8] else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                    }
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø­Ø¯Ø¯Ø§Ù‹
                    if result_dict["file_path"] and not os.path.exists(result_dict["file_path"]):
                        LOGGER(__name__).warning(f"âš ï¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {result_dict['file_path']}")
                        result_dict["file_path"] = None
                    
                    elapsed_time = time.time() - start_time
                    LOGGER(__name__).info(
                        f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ:\n"
                        f"   ğŸµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {result_dict['title']}\n"
                        f"   ğŸ‘¤ Ø§Ù„ÙÙ†Ø§Ù†: {result_dict['artist']}\n"
                        f"   ğŸ“ Ø§Ù„Ù…Ù„Ù: {'âœ… Ù…ÙˆØ¬ÙˆØ¯' if result_dict['file_path'] else 'âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}\n"
                        f"   â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {elapsed_time:.2f} Ø«Ø§Ù†ÙŠØ©"
                    )
                    
                    return result_dict
                    
                except Exception as e:
                    LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¨Ø­Ø«: {e}")
                    return None
            else:
                elapsed_time = time.time() - start_time
                LOGGER(__name__).info(f"ğŸ” Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ (â±ï¸ {elapsed_time:.2f}s)")
                return None
                
        except sqlite3.Error as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«: {e}")
            return None
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        LOGGER(__name__).error(
            f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ:\n"
            f"   ğŸ” Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}\n"
            f"   â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {elapsed_time:.2f} Ø«Ø§Ù†ÙŠØ©\n"
            f"   ğŸ“‹ Ø§Ù„Ø®Ø·Ø£: {str(e)}"
        )
        import traceback
        LOGGER(__name__).error(f"ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ÙƒØ§Ù…Ù„Ø©: {traceback.format_exc()}")
        return None
        
    finally:
        # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if conn:
            try:
                conn.close()
                LOGGER(__name__).debug("ğŸ”’ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            except Exception as e:
                LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")

async def sequential_external_search(query: str) -> Optional[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"""
    try:
        LOGGER(__name__).info(f"ğŸŒ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„: {query}")
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: YouTube Search
        try:
            LOGGER(__name__).info("ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© YouTube Search...")
            if YOUTUBE_SEARCH_AVAILABLE:
                search = YoutubeSearch(query, max_results=1)
                results = search.to_dict()
                
                if results:
                    result = results[0]
                    video_id = result.get('id', '')
                    
                    if video_id:
                        LOGGER(__name__).info(f"âœ… YouTube Search Ù†Ø¬Ø­: {result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                        return {
                            'id': video_id,
                            'title': result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                            'channel': result.get('channel', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                            'duration': result.get('duration', '0:00'),
                            'views': result.get('views', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                            'source': 'youtube_search'
                        }
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ YouTube Search ÙØ´Ù„: {e}")
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· (YouTube API + yt-dlp)
        try:
            LOGGER(__name__).info("ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· (YouTube API + yt-dlp)...")
            from .youtube_api_downloader import download_youtube_hybrid
            
            success, result = await download_youtube_hybrid(query, "downloads")
            if success and result:
                LOGGER(__name__).info(f"âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· Ù†Ø¬Ø­: {result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                return {
                    'id': result['video_id'],
                    'title': result['title'],
                    'channel': result.get('channel', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                    'duration': '0:00',  # Ø³ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† Ø§Ù„Ù…Ù„Ù
                    'views': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                    'source': 'hybrid_api_ytdlp',
                    'file_path': result['file_path'],
                    'thumbnail': result.get('thumbnail', ''),
                    'url': result['url']
                }
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· ÙØ´Ù„: {e}")
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: YouTube API Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
        try:
            LOGGER(__name__).info("ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© YouTube API Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ...")
            import config
            
            if hasattr(config, 'YOUTUBE_API_KEY') and config.YOUTUBE_API_KEY:
                # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© YouTube API Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ù‡Ù†Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
                pass
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ YouTube API ÙØ´Ù„: {e}")
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Invidious (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
        try:
            LOGGER(__name__).info("ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© Invidious...")
            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Invidious Ù‡Ù†Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
            pass
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Invidious ÙØ´Ù„: {e}")
        
        LOGGER(__name__).warning("âŒ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ")
        return None
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ: {e}")
        return None

async def send_local_cached_audio(message, cache_result: Dict, status_msg) -> bool:
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ"""
    try:
        LOGGER(__name__).info(f"ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ: {cache_result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        
        file_path = cache_result.get('file_path')
        
        if file_path and os.path.exists(file_path):
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
            await message.reply(
                file=file_path,
                message=f"âœ¦ @{config.BOT_USERNAME}",
                attributes=[
                    DocumentAttributeAudio(
                        duration=cache_result.get('duration', 0),
                        title=cache_result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                        performer=cache_result.get('artist', 'ZeMusic Bot')
                    )
                ]
            )
            
            await status_msg.delete()
            LOGGER(__name__).info("âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        else:
            LOGGER(__name__).warning("âš ï¸ Ù…Ù„Ù Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return False
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ: {e}")
        return False

async def send_telegram_cached_audio(message, telegram_result: Dict, status_msg) -> bool:
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠ Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    try:
        LOGGER(__name__).info(f"ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {telegram_result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        
        message_id = telegram_result.get('message_id')
        file_id = telegram_result.get('file_id')
        
        if file_id:
            # Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù€ file_id
            await message.reply(
                file=file_id,
                message=f"âœ¦ @{config.BOT_USERNAME}",
                attributes=[
                    DocumentAttributeAudio(
                        duration=telegram_result.get('duration', 0),
                        title=telegram_result.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                        performer=telegram_result.get('artist', 'ZeMusic Bot')
                    )
                ]
            )
            
            await status_msg.delete()
            LOGGER(__name__).info("âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        elif message_id:
            # Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
            import config
            cache_channel = config.CACHE_CHANNEL_ID
            
            await message.client.forward_messages(
                entity=message.chat_id,
                messages=message_id,
                from_peer=cache_channel
            )
            
            await status_msg.delete()
            LOGGER(__name__).info("âœ… ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        else:
            LOGGER(__name__).warning("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ file_id Ø£Ùˆ message_id ØµØ§Ù„Ø­")
            return False
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")
        return False

async def smart_download_and_send(message, video_info: Dict, status_msg) -> bool:
    """Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ cookies ÙˆØ­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø¯Ù‚ÙŠÙ‚Ø©"""
    start_time = time.time()
    downloaded_file = None
    
    try:
        LOGGER(__name__).info(f"â¬‡ï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù…Ø©")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù Ù…Ø­Ù…Ù„ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø·
        if video_info.get('source') == 'hybrid_api_ytdlp' and video_info.get('file_path'):
            hybrid_file_path = video_info.get('file_path')
            if os.path.exists(hybrid_file_path):
                LOGGER(__name__).info(f"âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ù…Ø­Ù…Ù„ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø·: {hybrid_file_path}")
                try:
                    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±Ø©
                    await status_msg.edit("ğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„...**")
                    
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù
                    file_size = os.path.getsize(hybrid_file_path)
                    duration = get_audio_duration(hybrid_file_path) if os.path.exists(hybrid_file_path) else 0
                    
                    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù
                    sent_message = await message.reply_audio(
                        audio=hybrid_file_path,
                        caption=f"ğŸµ **{video_info.get('title', 'Ø£ØºÙ†ÙŠØ©')}**\nğŸ‘¤ **{video_info.get('channel', 'Ù‚Ù†Ø§Ø©')}**",
                        duration=duration,
                        title=video_info.get('title', 'Ø£ØºÙ†ÙŠØ©'),
                        performer=video_info.get('channel', 'Ù‚Ù†Ø§Ø©')
                    )
                    
                    if sent_message:
                        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        await save_to_database_enhanced(
                            video_info.get('title', 'Ø£ØºÙ†ÙŠØ©'),
                            video_info.get('id', ''),
                            sent_message.audio.file_id,
                            duration,
                            video_info.get('channel', 'Ù‚Ù†Ø§Ø©'),
                            video_info.get('thumbnail', ''),
                            hybrid_file_path
                        )
                        
                        await status_msg.delete()
                        LOGGER(__name__).info("âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„Ø· Ø¨Ù†Ø¬Ø§Ø­")
                        return True
                        
                except Exception as e:
                    LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø®ØªÙ„Ø·: {e}")
                    # Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        if not video_info or not isinstance(video_info, dict):
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± ØµØ­ÙŠØ­Ø©")
            return False
            
        if not message:
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: ÙƒØ§Ø¦Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØºÙŠØ± Ù…ØªØ§Ø­")
            return False
            
        if not status_msg:
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
            return False
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©
        title = video_info.get('title', 'Ø£ØºÙ†ÙŠØ© ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©').strip()
        video_id = video_info.get('id', '').strip()
        duration_text = video_info.get('duration', '0:00').strip()
        channel = video_info.get('channel', 'Ù‚Ù†Ø§Ø© ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©').strip()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ video_id
        if not video_id:
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…ÙÙ‚ÙˆØ¯")
            return False
        
        LOGGER(__name__).info(
            f"ğŸ“‹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„:\n"
            f"   ğŸµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}\n"
            f"   ğŸ†” Ø§Ù„Ù…Ø¹Ø±Ù: {video_id}\n"
            f"   ğŸ‘¤ Ø§Ù„Ù‚Ù†Ø§Ø©: {channel}\n"
            f"   â±ï¸ Ø§Ù„Ù…Ø¯Ø©: {duration_text}"
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ø© Ø¥Ù„Ù‰ Ø«ÙˆØ§Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡
        duration = 0
        try:
            if ':' in duration_text:
                parts = duration_text.split(':')
                if len(parts) == 2:
                    duration = int(parts[0]) * 60 + int(parts[1])
                elif len(parts) == 3:
                    duration = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
                    
                LOGGER(__name__).debug(f"â±ï¸ ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ø©: {duration_text} â†’ {duration} Ø«Ø§Ù†ÙŠØ©")
                
        except (ValueError, IndexError) as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ø© '{duration_text}': {e}")
            duration = 0
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± yt-dlp
        if not yt_dlp:
            LOGGER(__name__).error("âŒ Ø®Ø·Ø£: Ù…ÙƒØªØ¨Ø© yt-dlp ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
            return False
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù cookies Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø¯Ù‚ÙŠÙ‚Ø©
        cookie_file = None
        try:
            await status_msg.edit("ğŸª **Ø¬Ø§Ø±ÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ­Ù…ÙŠÙ„...**")
            LOGGER(__name__).debug("ğŸª Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù cookies")
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† Ù…Ù„ÙØ§Øª cookies
            cookies_dir = Path("cookies")
            if cookies_dir.exists():
                cookie_files = list(cookies_dir.glob("*.txt"))
                if cookie_files:
                    # Ø§Ø®ØªÙŠØ§Ø± Ù…Ù„Ù Ø¹Ø´ÙˆØ§Ø¦ÙŠ
                    cookie_file = str(cookie_files[0])  # Ø£ÙˆÙ„ Ù…Ù„Ù Ù…ØªØ§Ø­
                    
                    if os.path.exists(cookie_file):
                        file_size = os.path.getsize(cookie_file)
                        LOGGER(__name__).info(f"âœ… ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù cookies: {cookie_file} ({file_size} bytes)")
                    else:
                        LOGGER(__name__).warning("âš ï¸ Ù…Ù„Ù cookies ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                        cookie_file = None
                else:
                    LOGGER(__name__).warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª cookies ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯")
                    cookie_file = None
            else:
                LOGGER(__name__).warning("âš ï¸ Ù…Ø¬Ù„Ø¯ cookies ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                cookie_file = None
            
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ cookies: {e}")
            cookie_file = None
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚
        try:
            downloads_dir = Path("downloads")
            downloads_dir.mkdir(exist_ok=True)
            LOGGER(__name__).debug(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª Ø¬Ø§Ù‡Ø²: {downloads_dir.absolute()}")
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª
            if not os.access(downloads_dir, os.W_OK):
                LOGGER(__name__).error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙ„Ø§Ø­ÙŠØ© ÙƒØªØ§Ø¨Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª")
                return False
                
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª: {e}")
            return False
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ MP3
        try:
            ydl_opts = get_ytdlp_opts(cookie_file)
            ydl_opts['outtmpl'] = f'downloads/{video_id}.%(ext)s'
            
            # Ø¥Ø¶Ø§ÙØ© cookies Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if cookie_file and os.path.exists(cookie_file):
                ydl_opts['cookiefile'] = cookie_file
                LOGGER(__name__).info("ğŸª ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ù cookies Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„")
            else:
                LOGGER(__name__).info("ğŸš« Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙˆÙ† cookies")
            
            LOGGER(__name__).debug(f"âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª yt-dlp: {ydl_opts}")
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}")
            return False
        
        # ØªØ­Ø¯ÙŠØ« Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø©
        try:
            await status_msg.edit("ğŸ“¥ **Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† YouTube...**")
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø©: {e}")
        
        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                video_url = f"https://www.youtube.com/watch?v={video_id}"
                info = ydl.extract_info(video_url, download=True)
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„
                downloaded_file = None
                for ext in ['mp3', 'webm', 'm4a', 'ogg', 'opus']:
                    file_path = f'downloads/{video_id}.{ext}'
                    if os.path.exists(file_path):
                        downloaded_file = file_path
                        break
                
                if not downloaded_file:
                    LOGGER(__name__).error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„")
                    return False
                
                # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                thumb_path = None
                try:
                    if info and 'thumbnail' in info and info['thumbnail']:
                        thumb_path = await download_thumbnail(info['thumbnail'], title, video_id)
                    elif info and 'thumbnails' in info and info['thumbnails']:
                        # Ø£Ø®Ø° Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø© Ù…ØªØ§Ø­Ø©
                        best_thumb = None
                        for thumb in info['thumbnails']:
                            if thumb.get('url'):
                                best_thumb = thumb['url']
                        if best_thumb:
                            thumb_path = await download_thumbnail(best_thumb, title, video_id)
                except Exception as thumb_error:
                    LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {thumb_error}")
                
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù
                await status_msg.edit("ğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                
                try:
                    LOGGER(__name__).info(f"ğŸ“¤ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù: {downloaded_file}")
                    audio_message = await message.reply(
                        file=downloaded_file,
                        message=f"âœ¦ @{config.BOT_USERNAME}",
                        thumb=thumb_path,
                        attributes=[
                            DocumentAttributeAudio(
                                duration=duration,
                                title=title,
                                performer=channel
                            )
                        ]
                    )
                    LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­: {audio_message.id}")
                except Exception as send_error:
                    LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù: {send_error}")
                    raise send_error
                
                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
                await save_to_cache(video_id, title, channel, duration, downloaded_file, audio_message, thumb_path)
                
                # Ø­Ø°Ù Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø©
                try:
                    await status_msg.delete()
                except:
                    pass
                
                # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
                try:
                    os.remove(downloaded_file)
                except:
                    pass
                
                # Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                if thumb_path and os.path.exists(thumb_path):
                    try:
                        os.remove(thumb_path)
                    except:
                        pass
                
                LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ ÙˆØ­ÙØ¸ Ø§Ù„Ø£ØºÙ†ÙŠØ©: {title}")
                return True
                
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ cookies: {e}")
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙˆÙ† cookies
            LOGGER(__name__).info("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙˆÙ† cookies...")
            await status_msg.edit("ğŸ”„ **Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙŠÙ„Ø©...**")
            
            try:
                ydl_opts_no_cookies = get_ytdlp_opts()
                ydl_opts_no_cookies['outtmpl'] = f'downloads/{video_id}_nocookies.%(ext)s'
                
                with yt_dlp.YoutubeDL(ydl_opts_no_cookies) as ydl:
                    video_url = f"https://www.youtube.com/watch?v={video_id}"
                    info = ydl.extract_info(video_url, download=True)
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù…Ù„
                    downloaded_file = None
                    for ext in ['mp3', 'webm', 'm4a', 'ogg', 'opus']:
                        file_path = f'downloads/{video_id}_nocookies.{ext}'
                        if os.path.exists(file_path):
                            downloaded_file = file_path
                            break
                    
                    if downloaded_file:
                        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                        thumb_path = None
                        try:
                            if info and 'thumbnail' in info and info['thumbnail']:
                                thumb_path = await download_thumbnail(info['thumbnail'], title, video_id)
                            elif info and 'thumbnails' in info and info['thumbnails']:
                                # Ø£Ø®Ø° Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø© Ù…ØªØ§Ø­Ø©
                                best_thumb = None
                                for thumb in info['thumbnails']:
                                    if thumb.get('url'):
                                        best_thumb = thumb['url']
                                if best_thumb:
                                    thumb_path = await download_thumbnail(best_thumb, title, video_id)
                        except Exception as thumb_error:
                            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©: {thumb_error}")
                        
                        await status_msg.edit("ğŸ“¤ **Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„...**")
                        
                        audio_message = await message.reply(
                            file=downloaded_file,
                            message=f"âœ¦ @{config.BOT_USERNAME}",
                            thumb=thumb_path,
                            attributes=[
                                DocumentAttributeAudio(
                                    duration=duration,
                                    title=title,
                                    performer=channel
                                )
                            ]
                        )
                        
                        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
                        await save_to_cache(video_id, title, channel, duration, downloaded_file, audio_message, thumb_path)
                        
                        try:
                            await status_msg.delete()
                        except:
                            pass
                        
                        try:
                            os.remove(downloaded_file)
                        except:
                            pass
                        
                        # Ø­Ø°Ù Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                        if thumb_path and os.path.exists(thumb_path):
                            try:
                                os.remove(thumb_path)
                            except:
                                pass
                        
                        LOGGER(__name__).info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ØºÙ†ÙŠØ© Ø¨Ø¯ÙˆÙ† cookies: {title}")
                        return True
                        
            except Exception as e2:
                LOGGER(__name__).error(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø¯ÙˆÙ† cookies Ø£ÙŠØ¶Ø§Ù‹: {e2}")
                return False
        
        return False
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ: {e}")
        return False

async def save_to_cache(video_id: str, title: str, artist: str, duration: int, file_path: str, audio_message, thumb_path: str = None) -> bool:
    """Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆÙ‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    try:
        LOGGER(__name__).info(f"ğŸ’¾ Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´: {title}")
        
        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        try:
            conn = sqlite3.connect(DATABASE_PATH)
            cursor = conn.cursor()
            
            # Ø¥Ø¯Ø±Ø§Ø¬ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
            cursor.execute("""
                INSERT OR REPLACE INTO cached_audio 
                (video_id, title, artist, duration, file_path, thumb, message_id, keywords, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
            """, (
                video_id,
                title,
                artist,
                duration,
                file_path,
                None,  # thumb
                getattr(audio_message, 'id', None),
                f"{title} {artist}".lower(),  # keywords
            ))
            
            conn.commit()
            conn.close()
            
            LOGGER(__name__).info("âœ… ØªÙ… Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©")
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        
        # Ø­ÙØ¸ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©)
        try:
            import config
            from ZeMusic.core.telethon_client import telethon_manager
            
            if hasattr(config, 'CACHE_CHANNEL_ID') and config.CACHE_CHANNEL_ID:
                LOGGER(__name__).info(f"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...")
                
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù„Ù„Ø­ÙØ¸
                result_data = {
                    'title': title,
                    'uploader': artist,
                    'duration': duration,
                    'source': 'YouTube',
                    'elapsed': 0
                }
                
                # Ø­ÙØ¸ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… save_to_smart_cache
                if telethon_manager and telethon_manager.bot_client:
                    saved = await save_to_smart_cache(
                        telethon_manager.bot_client, 
                        file_path, 
                        result_data, 
                        f"{title} {artist}",
                        thumb_path  # ØªÙ…Ø±ÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØµØºØ±Ø©
                    )
                    if saved:
                        LOGGER(__name__).info("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†")
                    else:
                        LOGGER(__name__).warning("âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†")
                
        except Exception as e:
            LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")
        
        return True
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø´: {e}")
        return False

LOGGER(__name__).info("ğŸš€ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø®Ø§Ø±Ù‚ Ø§Ù„Ù…ØªØ·ÙˆØ± V2")

# Ø¥Ø¶Ø§ÙØ© Ù†Ø¸Ø§Ù… Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¯ÙˆØ±ÙŠ Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
LAST_CHANNEL_SYNC = 0
CHANNEL_SYNC_INTERVAL = 3600  # ÙƒÙ„ Ø³Ø§Ø¹Ø©

async def sync_channel_to_database(bot_client, force_sync: bool = False) -> Dict:
    """Ù…Ø²Ø§Ù…Ù†Ø© Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ"""
    global LAST_CHANNEL_SYNC
    
    current_time = time.time()
    
    # ÙØ­Øµ Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„Ù…Ø²Ø§Ù…Ù†Ø©
    if not force_sync and (current_time - LAST_CHANNEL_SYNC) < CHANNEL_SYNC_INTERVAL:
        return {'skipped': True, 'reason': 'Ù„Ù… ØªØ­Ù† Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨Ø¹Ø¯'}
    
    try:
        import config
        
        if not hasattr(config, 'CACHE_CHANNEL_ID') or not config.CACHE_CHANNEL_ID:
            return {'error': 'Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©'}
        
        cache_channel = config.CACHE_CHANNEL_ID
        LOGGER(__name__).info(f"âš ï¸ Ù…Ø²Ø§Ù…Ù†Ø© Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù…Ø¹Ø·Ù„Ø© Ù…Ø¤Ù‚ØªØ§Ù‹ (Ù‚ÙŠÙˆØ¯ API Ù„Ù„Ø¨ÙˆØªØ§Øª)")
        return {'error': 'Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ø¹Ø·Ù„Ø© Ù…Ø¤Ù‚ØªØ§Ù‹'}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
        sync_stats = {
            'processed': 0,
            'added': 0,
            'updated': 0,
            'errors': 0,
            'start_time': current_time
        }
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± message_id ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        cursor.execute("SELECT MAX(message_id) FROM channel_index")
        last_db_message_id = cursor.fetchone()[0] or 0
        
        LOGGER(__name__).info(f"ğŸ“Š Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {last_db_message_id}")
        
        # ÙØ­Øµ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ù‚Ù†Ø§Ø©
        new_messages_found = 0
        batch_size = 100
        
        async for message in bot_client.iter_messages(cache_channel, limit=1000):
            if not (message.text and message.file):
                continue
                
            sync_stats['processed'] += 1
            
            # ØªØ®Ø·ÙŠ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„
            if message.id <= last_db_message_id:
                continue
                
            new_messages_found += 1
            
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø©
                title = extract_title_from_cache_text(message.text)
                uploader = extract_uploader_from_cache_text(message.text)
                duration = extract_duration_from_cache_text(message.text)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ø§Ø´ Ø§Ù„Ø¨Ø­Ø« Ù…Ù† Ø§Ù„Ù†Øµ Ø¥Ù† ÙˆØ¬Ø¯
                import re
                hash_match = re.search(r'Ù‡Ø§Ø´ Ø§Ù„Ø¨Ø­Ø«.*?`([a-f0-9]+)`', message.text)
                search_hash = hash_match.group(1) if hash_match else None
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ Ø¬Ø¯ÙŠØ¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯
                if not search_hash:
                    title_normalized = normalize_search_text(title)
                    uploader_normalized = normalize_search_text(uploader)
                    search_data = f"{title_normalized}|{uploader_normalized}"
                    search_hash = hashlib.md5(search_data.encode()).hexdigest()[:12]
                
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
                title_normalized = normalize_search_text(title)
                uploader_normalized = normalize_search_text(uploader)
                
                # Ø¥Ù†Ø´Ø§Ø¡ vector Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                keywords_vector = f"{title_normalized} {uploader_normalized}"
                
                # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³Ø¬Ù„
                cursor.execute("SELECT id FROM channel_index WHERE message_id = ?", (message.id,))
                existing = cursor.fetchone()
                
                if existing:
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
                    cursor.execute("""
                        UPDATE channel_index 
                        SET file_id = ?, title_normalized = ?, artist_normalized = ?, 
                            keywords_vector = ?, original_title = ?, original_artist = ?, 
                            duration = ?, file_size = ?, search_hash = ?
                        WHERE message_id = ?
                    """, (
                        message.file.id, title_normalized, uploader_normalized,
                        keywords_vector, title, uploader, duration,
                        message.file.size or 0, search_hash, message.id
                    ))
                    sync_stats['updated'] += 1
                else:
                    # Ø¥Ø¶Ø§ÙØ© Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
                    cursor.execute("""
                        INSERT INTO channel_index 
                        (message_id, file_id, file_unique_id, search_hash, title_normalized, 
                         artist_normalized, keywords_vector, original_title, original_artist, 
                         duration, file_size, access_count, popularity_rank)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0, 0.5)
                    """, (
                        message.id, message.file.id, getattr(message.file, 'unique_id', None),
                        search_hash, title_normalized, uploader_normalized, keywords_vector,
                        title, uploader, duration, message.file.size or 0
                    ))
                    sync_stats['added'] += 1
                
                # Ø­ÙØ¸ Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
                if sync_stats['processed'] % batch_size == 0:
                    conn.commit()
                    LOGGER(__name__).info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø¯ÙØ¹Ø©: {sync_stats['processed']} Ø±Ø³Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©")
                
            except Exception as msg_error:
                sync_stats['errors'] += 1
                LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© {message.id}: {msg_error}")
                continue
        
        # Ø­ÙØ¸ Ù†Ù‡Ø§Ø¦ÙŠ
        conn.commit()
        conn.close()
        
        # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©
        LAST_CHANNEL_SYNC = current_time
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        sync_stats['duration'] = time.time() - current_time
        sync_stats['new_messages'] = new_messages_found
        
        LOGGER(__name__).info(
            f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù‚Ù†Ø§Ø©: "
            f"Ù…Ø¹Ø§Ù„Ø¬={sync_stats['processed']} | "
            f"Ø¬Ø¯ÙŠØ¯={sync_stats['added']} | "
            f"Ù…Ø­Ø¯Ø«={sync_stats['updated']} | "
            f"Ø£Ø®Ø·Ø§Ø¡={sync_stats['errors']} | "
            f"Ù…Ø¯Ø©={sync_stats['duration']:.2f}s"
        )
        
        return sync_stats
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù‚Ù†Ø§Ø©: {e}")
        return {'error': str(e)}

async def auto_sync_channel_if_needed(bot_client):
    """ÙØ­Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø­Ø§Ø¬Ø© Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù‚Ù†Ø§Ø©"""
    try:
        # ÙØ­Øµ Ø¢Ø®Ø± Ù…Ø²Ø§Ù…Ù†Ø©
        current_time = time.time()
        if (current_time - LAST_CHANNEL_SYNC) > CHANNEL_SYNC_INTERVAL:
            LOGGER(__name__).info("ğŸ”„ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...")
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
            asyncio.create_task(sync_channel_to_database(bot_client, force_sync=False))
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©: {e}")

async def force_channel_sync_handler(event):
    """Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± Ø§Ù„Ù…Ø·ÙˆØ± Ù„Ø¥Ø¬Ø¨Ø§Ø± Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù‚Ù†Ø§Ø©"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        await event.reply("ğŸ”„ **Ø¨Ø¯Ø¡ Ù…Ø²Ø§Ù…Ù†Ø© Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...**")
        
        result = await sync_channel_to_database(event.client, force_sync=True)
        
        if 'error' in result:
            await event.reply(f"âŒ **Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©:** {result['error']}")
        elif 'skipped' in result:
            await event.reply(f"â­ï¸ **ØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©:** {result['reason']}")
        else:
            response = f"""âœ… **Ø§ÙƒØªÙ…Ù„Øª Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù‚Ù†Ø§Ø©!**

ğŸ“Š **Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:**
â€¢ Ø±Ø³Ø§Ø¦Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø©: {result['processed']}
â€¢ Ø³Ø¬Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©: {result['added']}
â€¢ Ø³Ø¬Ù„Ø§Øª Ù…Ø­Ø¯Ø«Ø©: {result['updated']}
â€¢ Ø£Ø®Ø·Ø§Ø¡: {result['errors']}
â€¢ Ø±Ø³Ø§Ø¦Ù„ Ø¬Ø¯ÙŠØ¯Ø©: {result['new_messages']}
â€¢ Ø§Ù„Ù…Ø¯Ø©: {result['duration']:.2f}s

ğŸ’¾ ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­"""
            
            await event.reply(response)
        
    except Exception as e:
        await event.reply(f"âŒ **Ø®Ø·Ø£:** {e}")

# ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù„ÙŠØ´Ù…Ù„ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©

# Ø¥Ø¶Ø§ÙØ© Ø¯Ø§Ù„Ø© ÙØ­Øµ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
async def verify_cache_channel(bot_client) -> Dict:
    """ÙØ­Øµ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    try:
        import config
        
        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„ØªØ¹Ø±ÙŠÙ
        if not hasattr(config, 'CACHE_CHANNEL_ID') or not config.CACHE_CHANNEL_ID:
            return {
                'status': 'error',
                'message': 'Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ config.py',
                'solution': 'ØªØ£ÙƒØ¯ Ù…Ù† ØªØ¹ÙŠÙŠÙ† CACHE_CHANNEL_USERNAME ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©'
            }
        
        cache_channel = config.CACHE_CHANNEL_ID
        
        # ÙØ­Øµ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ø±Ù
        channel_type = "unknown"
        if cache_channel.startswith('@'):
            channel_type = "username"
        elif cache_channel.startswith('-100'):
            channel_type = "supergroup_id"
        elif cache_channel.startswith('-'):
            channel_type = "group_id"
        elif cache_channel.isdigit():
            channel_type = "user_id"
        
        LOGGER(__name__).info(f"ğŸ” ÙØ­Øµ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {cache_channel} (Ù†ÙˆØ¹: {channel_type})")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‚Ù†Ø§Ø©
        try:
            entity = await bot_client.get_entity(cache_channel)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ù†Ø§Ø©
            channel_info = {
                'status': 'success',
                'channel_id': cache_channel,
                'channel_type': channel_type,
                'entity_id': entity.id,
                'title': getattr(entity, 'title', 'Unknown'),
                'username': getattr(entity, 'username', None),
                'participants_count': getattr(entity, 'participants_count', 0),
                'is_channel': hasattr(entity, 'broadcast'),
                'is_megagroup': getattr(entity, 'megagroup', False),
                'access_hash': getattr(entity, 'access_hash', None)
            }
            
            # ÙØ­Øµ ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ø¥Ø±Ø³Ø§Ù„
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±
                test_message = await bot_client.send_message(
                    entity, 
                    "ğŸ§ª **Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ**\n\nâœ… Ø§Ù„Ø¨ÙˆØª ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!\n\nğŸ—‘ï¸ Ø³ÙŠØªÙ… Ø­Ø°Ù Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø®Ù„Ø§Ù„ 10 Ø«ÙˆØ§Ù†..."
                )
                
                # Ø­Ø°Ù Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø¨Ø¹Ø¯ 10 Ø«ÙˆØ§Ù†
                await asyncio.sleep(10)
                await test_message.delete()
                
                channel_info['can_send'] = True
                channel_info['permissions'] = 'full'
                
            except Exception as perm_error:
                channel_info['can_send'] = False
                channel_info['permissions'] = 'limited'
                channel_info['permission_error'] = str(perm_error)
            
            # ÙØ­Øµ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
            try:
                message_count = 0
                audio_count = 0
                
                async for message in bot_client.iter_messages(entity, limit=100):
                    message_count += 1
                    if message.file and message.file.mime_type and 'audio' in message.file.mime_type:
                        audio_count += 1
                
                channel_info['recent_messages'] = message_count
                channel_info['recent_audio_files'] = audio_count
                
            except Exception as count_error:
                channel_info['recent_messages'] = 0
                channel_info['recent_audio_files'] = 0
                channel_info['count_error'] = str(count_error)
            
            LOGGER(__name__).info(f"âœ… Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ù…ØªØ§Ø­Ø©: {channel_info['title']}")
            return channel_info
            
        except Exception as access_error:
            return {
                'status': 'error',
                'channel_id': cache_channel,
                'channel_type': channel_type,
                'message': f'Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {access_error}',
                'solutions': [
                    'ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙˆØª Ø¹Ø¶Ùˆ ÙÙŠ Ø§Ù„Ù‚Ù†Ø§Ø©',
                    'ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙˆØª Ù„Ø¯ÙŠÙ‡ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø¥Ø±Ø³Ø§Ù„',
                    'ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ù…Ø¹Ø±Ù/ÙŠÙˆØ²Ø± Ø§Ù„Ù‚Ù†Ø§Ø©',
                    'ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚Ù†Ø§Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆÙ„ÙŠØ³Øª Ù…Ø­Ø°ÙˆÙØ©'
                ]
            }
            
    except Exception as e:
        return {
            'status': 'error',
            'message': f'Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}',
            'solution': 'ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª config.py'
        }

async def cache_channel_info_handler(event):
    """Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± Ø§Ù„Ù…Ø·ÙˆØ± Ù„Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        await event.reply("ğŸ” **Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...**")
        
        # ÙØ­Øµ Ø§Ù„Ù‚Ù†Ø§Ø©
        result = await verify_cache_channel(event.client)
        
        if result['status'] == 'error':
            error_msg = f"âŒ **Ø®Ø·Ø£ ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†**\n\n"
            error_msg += f"**Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** {result['message']}\n\n"
            
            if 'solutions' in result:
                error_msg += "**Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:**\n"
                for i, solution in enumerate(result['solutions'], 1):
                    error_msg += f"{i}. {solution}\n"
            elif 'solution' in result:
                error_msg += f"**Ø§Ù„Ø­Ù„:** {result['solution']}\n"
            
            await event.reply(error_msg)
        else:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù†Ø¬Ø§Ø­
            success_msg = f"âœ… **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†**\n\n"
            success_msg += f"ğŸ·ï¸ **Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:** {result['title']}\n"
            success_msg += f"ğŸ†” **Ø§Ù„Ù…Ø¹Ø±Ù:** `{result['channel_id']}`\n"
            success_msg += f"ğŸ”¢ **ID Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ:** `{result['entity_id']}`\n"
            
            if result.get('username'):
                success_msg += f"ğŸ‘¤ **Ø§Ù„ÙŠÙˆØ²Ø±:** @{result['username']}\n"
            
            success_msg += f"ğŸ“Š **Ø§Ù„Ù†ÙˆØ¹:** {result['channel_type']}\n"
            success_msg += f"ğŸ‘¥ **Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡:** {result.get('participants_count', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}\n"
            success_msg += f"ğŸ“º **Ù‚Ù†Ø§Ø©:** {'Ù†Ø¹Ù…' if result.get('is_channel') else 'Ù„Ø§'}\n"
            success_msg += f"ğŸ”“ **Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø©:** {'Ù†Ø¹Ù…' if result.get('is_megagroup') else 'Ù„Ø§'}\n\n"
            
            # ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ø¥Ø±Ø³Ø§Ù„
            if result.get('can_send'):
                success_msg += "âœ… **Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª:** Ø§Ù„Ø¨ÙˆØª ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„\n"
            else:
                success_msg += "âŒ **Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª:** Ø§Ù„Ø¨ÙˆØª Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„\n"
                if 'permission_error' in result:
                    success_msg += f"**Ø§Ù„Ø³Ø¨Ø¨:** {result['permission_error']}\n"
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            success_msg += f"\nğŸ“ˆ **Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰:**\n"
            success_msg += f"â€¢ Ø±Ø³Ø§Ø¦Ù„ Ø­Ø¯ÙŠØ«Ø©: {result.get('recent_messages', 0)}\n"
            success_msg += f"â€¢ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ©: {result.get('recent_audio_files', 0)}\n"
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            try:
                conn = sqlite3.connect(DB_FILE)
                cursor = conn.cursor()
                
                cursor.execute("SELECT COUNT(*) FROM channel_index")
                total_cached = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM channel_index WHERE last_accessed > datetime('now', '-7 days')")
                recent_accessed = cursor.fetchone()[0]
                
                conn.close()
                
                success_msg += f"\nğŸ’¾ **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**\n"
                success_msg += f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­ÙÙˆØ¸: {total_cached}\n"
                success_msg += f"â€¢ Ø§Ø³ØªÙØ®Ø¯Ù… Ù…Ø¤Ø®Ø±Ø§Ù‹: {recent_accessed}\n"
                
            except Exception as db_error:
                success_msg += f"\nâš ï¸ **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©\n"
            
            await event.reply(success_msg)
        
    except Exception as e:
        await event.reply(f"âŒ **Ø®Ø·Ø£:** {e}")

async def test_cache_channel_handler(event):
    """Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± Ø§Ù„Ù…Ø·ÙˆØ± Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        await event.reply("ğŸ§ª **Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...**")
        
        # ÙØ­Øµ Ø§Ù„Ù‚Ù†Ø§Ø© Ø£ÙˆÙ„Ø§Ù‹
        result = await verify_cache_channel(event.client)
        
        if result['status'] == 'error':
            await event.reply(f"âŒ **ÙØ´Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:** {result['message']}")
            return
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø­ÙØ¸ Ù…Ù„Ù ØªØ¬Ø±ÙŠØ¨ÙŠ
        import tempfile
        import os
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØµÙˆØªÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_file:
            # ÙƒØªØ§Ø¨Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© (Ù…Ù„Ù ÙØ§Ø±Øº)
            temp_file.write(b'test audio data')
            temp_path = temp_file.name
        
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ
            test_result = {
                'title': 'Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ',
                'uploader': 'ZeMusic Test Bot',
                'duration': 30,
                'file_size': 1024,
                'source': 'test_system',
                'elapsed': 0.5
            }
            
            success = await save_to_smart_cache(
                event.client, 
                temp_path, 
                test_result, 
                'Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…',
                None  # Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙˆØ±Ø© Ù…ØµØºØ±Ø© ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            )
            
            if success:
                await event.reply("âœ… **Ù†Ø¬Ø­ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±!**\n\nğŸ¯ ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ù†Ø¬Ø§Ø­\nğŸ’¾ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø«Ø©\nğŸš€ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„")
            else:
                await event.reply("âŒ **ÙØ´Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:** Ù„Ù… ÙŠØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ")
            
        finally:
            # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
    except Exception as e:
        await event.reply(f"âŒ **Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:** {e}")
        import traceback
        LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù‚Ù†Ø§Ø©: {traceback.format_exc()}")

# ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù„ÙŠØ´Ù…Ù„ ÙØ­Øµ Ø§Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ

async def smart_download_handler(event):
    """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙˆØ±ÙŠ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ Ù…Ø²Ø§Ù…Ù†Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ÙˆÙØ­Øµ Ø§Ù„Ù‚Ù†Ø§Ø©"""
    start_time = time.time()
    user_id = event.sender_id
    
    try:
        # ØªØªØ¨Ø¹ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª (Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙÙ‚Ø·)
        await check_rate_limit(user_id)
        
        # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ù‡ÙŠØ£Ø©
        await ensure_database_initialized()
        
        # ÙØ­Øµ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ (ÙƒÙ„ 50 Ø·Ù„Ø¨)
        if PERFORMANCE_STATS['total_requests'] % 50 == 0:
            asyncio.create_task(verify_cache_channel_periodic(event.client))
        
        # Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† (ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©)
        asyncio.create_task(auto_sync_channel_if_needed(event.client))
        
        # ØªÙ†Ø¸ÙŠÙ Ø¯ÙˆØ±ÙŠ Ù„Ù„ÙƒÙˆÙƒÙŠØ² Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø© (ÙƒÙ„ 100 Ø·Ù„Ø¨)
        if PERFORMANCE_STATS['total_requests'] % 100 == 0:
            cleanup_blocked_cookies()
        
        # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ (ÙƒÙ„ 50 Ø·Ù„Ø¨)
        if PERFORMANCE_STATS['total_requests'] % 50 == 0:
            log_performance_stats()
        
        # ÙØ­Øµ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª
        chat_id = event.chat_id
        if chat_id > 0:  # Ù…Ø­Ø§Ø¯Ø«Ø© Ø®Ø§ØµØ©
            if not await is_search_enabled1():
                await event.reply("âŸ¡ Ø¹Ø°Ø±Ø§Ù‹ Ø¹Ø²ÙŠØ²ÙŠ Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨ Ù…Ø¹Ø·Ù„ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø·ÙˆØ±")
                return
        else:  # Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ùˆ Ù‚Ù†Ø§Ø©
            if not await is_search_enabled(chat_id):
                await event.reply("âŸ¡ Ø¹Ø°Ø±Ø§Ù‹ Ø¹Ø²ÙŠØ²ÙŠ Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨ Ù…Ø¹Ø·Ù„ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø·ÙˆØ±")
                return
                
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        LOGGER(__name__).info(f"ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø°ÙƒÙŠØ© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} - Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {len(active_downloads)}")
        
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø­Ù…ÙˆÙ„Ø© Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
        await update_performance_stats(False, time.time() - start_time)
        return
    
    # ØªÙ†Ø¸ÙŠÙ Ø¯ÙˆØ±ÙŠ Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (ÙƒÙ„ 50 Ø·Ù„Ø¨)
    # if len(active_downloads) % 50 == 0:
    #     asyncio.create_task(cleanup_old_downloads())
    
    # ØªØ­ÙƒÙ… Ø°ÙƒÙŠ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
    current_downloads = len(active_downloads)
    
    if current_downloads < MAX_CONCURRENT_DOWNLOADS:
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙˆØ±ÙŠØ© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        asyncio.create_task(process_unlimited_download_enhanced(event, user_id, start_time))
        LOGGER(__name__).info(f"âš¡ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} - Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {current_downloads + 1}")
    else:
        # Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ²Ù†Ø§ Ø§Ù„Ø­Ø¯ØŒ Ù†Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø«Ù… Ù†Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
        LOGGER(__name__).info(f"â³ ØªØ£Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ - Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {current_downloads} (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰: {MAX_CONCURRENT_DOWNLOADS})")
        
        async def delayed_process():
            await asyncio.sleep(0.5)  # Ø§Ù†ØªØ¸Ø§Ø± Ù†ØµÙ Ø«Ø§Ù†ÙŠØ©
            if len(active_downloads) < MAX_CONCURRENT_DOWNLOADS:
                await process_unlimited_download_enhanced(event, user_id, start_time)
            else:
                # Ø¥Ø°Ø§ Ù„Ø§ ÙŠØ²Ø§Ù„ Ù…Ø²Ø¯Ø­Ù…Ø§Ù‹ØŒ Ù†Ù†Ø´Ø¦ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø£ÙŠ Ø­Ø§Ù„
                asyncio.create_task(process_unlimited_download_enhanced(event, user_id, start_time))
        
        asyncio.create_task(delayed_process())

async def cleanup_old_downloads():
    """ØªÙ†Ø¸ÙŠÙ Ø¯ÙˆØ±ÙŠ Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù…Ù†Ø¹ ØªØ±Ø§ÙƒÙ…Ù‡Ø§"""
    try:
        current_time = time.time()
        old_tasks = []
        
        for task_id, task_info in active_downloads.items():
            # Ø¥Ø°Ø§ Ù…Ø±Øª Ø£ÙƒØ«Ø± Ù…Ù† 10 Ø¯Ù‚Ø§Ø¦Ù‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŒ Ø§Ø­Ø°ÙÙ‡Ø§
            if current_time - task_info.get('start_time', current_time) > 600:
                old_tasks.append(task_id)
        
        for task_id in old_tasks:
            del active_downloads[task_id]
            LOGGER(__name__).info(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø¹Ù…Ù„ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©: {task_id}")
            
        if old_tasks:
            LOGGER(__name__).info(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {len(old_tasks)} Ø¹Ù…Ù„ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø© - Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {len(active_downloads)}")
            
    except Exception as e:
        LOGGER(__name__).warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: {e}")

async def verify_cache_channel_periodic(bot_client):
    """ÙØ­Øµ Ø¯ÙˆØ±ÙŠ Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    try:
        result = await verify_cache_channel(bot_client)
        
        if result['status'] == 'error':
            LOGGER(__name__).warning(f"âš ï¸ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {result['message']}")
        else:
            LOGGER(__name__).info(f"âœ… Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ: {result['title']}")
            
    except Exception as e:
        LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¯ÙˆØ±ÙŠ Ù„Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†: {e}")

# Ø¥Ø¶Ø§ÙØ© Ø¯Ø§Ù„Ø© Ù„Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ù…Ù„Ø©
async def system_status_handler(event):
    """Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± Ø§Ù„Ù…Ø·ÙˆØ± Ù„Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
    import config
    if event.sender_id != config.OWNER_ID:
        return
    
    try:
        await event.reply("ğŸ“Š **Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ù…Ù„Ø©...**")
        
        # ÙØ­Øµ Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
        cache_status = await verify_cache_channel(event.client)
        
        # ÙØ­Øµ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        try:
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM channel_index")
            total_cached = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM channel_index WHERE last_accessed > datetime('now', '-1 day')")
            daily_accessed = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM channel_index WHERE created_at > datetime('now', '-1 day')")
            daily_added = cursor.fetchone()[0]
            
            cursor.execute("SELECT AVG(popularity_rank) FROM channel_index")
            avg_popularity = cursor.fetchone()[0] or 0
            
            conn.close()
            
            db_status = {
                'working': True,
                'total_cached': total_cached,
                'daily_accessed': daily_accessed,
                'daily_added': daily_added,
                'avg_popularity': avg_popularity
            }
            
        except Exception as db_error:
            db_status = {
                'working': False,
                'error': str(db_error)
            }
        
        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²
        cookies_stats = get_cookies_statistics()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
        status_msg = "ğŸ“Š **Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ù…Ù„Ø©**\n\n"
        
        # Ø­Ø§Ù„Ø© Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
        if cache_status['status'] == 'success':
            status_msg += f"âœ… **Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†:** ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ\n"
            status_msg += f"   ğŸ“ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {cache_status['title']}\n"
            status_msg += f"   ğŸµ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ©: {cache_status.get('recent_audio_files', 0)}\n"
            status_msg += f"   ğŸ“¤ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø¥Ø±Ø³Ø§Ù„: {'âœ…' if cache_status.get('can_send') else 'âŒ'}\n"
        else:
            status_msg += f"âŒ **Ù‚Ù†Ø§Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†:** Ù…Ø´ÙƒÙ„Ø©\n"
            status_msg += f"   âš ï¸ {cache_status['message']}\n"
        
        status_msg += "\n"
        
        # Ø­Ø§Ù„Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if db_status['working']:
            status_msg += f"âœ… **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ\n"
            status_msg += f"   ğŸ’¾ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­ÙÙˆØ¸: {db_status['total_cached']}\n"
            status_msg += f"   ğŸ“ˆ Ø§Ø³ØªÙØ®Ø¯Ù… Ø§Ù„ÙŠÙˆÙ…: {db_status['daily_accessed']}\n"
            status_msg += f"   â• Ø£ÙØ¶ÙŠÙ Ø§Ù„ÙŠÙˆÙ…: {db_status['daily_added']}\n"
            status_msg += f"   â­ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©: {db_status['avg_popularity']:.2f}\n"
        else:
            status_msg += f"âŒ **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** Ù…Ø´ÙƒÙ„Ø©\n"
            status_msg += f"   âš ï¸ {db_status['error']}\n"
        
        status_msg += "\n"
        
        # Ø­Ø§Ù„Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²
        if cookies_stats:
            status_msg += f"ğŸª **Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²:**\n"
            status_msg += f"   ğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {cookies_stats.get('total', 0)}\n"
            status_msg += f"   âœ… Ø§Ù„Ù…ØªØ§Ø­: {cookies_stats.get('available', 0)}\n"
            status_msg += f"   ğŸš« Ø§Ù„Ù…Ø­Ø¸ÙˆØ±: {cookies_stats.get('blocked', 0)}\n"
            status_msg += f"   ğŸ† Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹: {cookies_stats.get('most_used_file', 'Ù„Ø§ ÙŠÙˆØ¬Ø¯')}\n"
        else:
            status_msg += f"âŒ **Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆÙƒÙŠØ²:** ØºÙŠØ± Ù…ØªØ§Ø­Ø©\n"
        
        status_msg += "\n"
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        stats = PERFORMANCE_STATS
        success_rate = (stats['successful_downloads'] / max(stats['total_requests'], 1)) * 100
        cache_hit_rate = (stats['cache_hits'] / max(stats['total_requests'], 1)) * 100
        
        status_msg += f"âš¡ **Ø§Ù„Ø£Ø¯Ø§Ø¡:**\n"
        status_msg += f"   ğŸ”¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø¨Ø§Øª: {stats['total_requests']}\n"
        status_msg += f"   âœ… Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:.1f}%\n"
        status_msg += f"   ğŸ’¾ Ù†Ø³Ø¨Ø© Ø§Ù„ÙƒØ§Ø´: {cache_hit_rate:.1f}%\n"
        status_msg += f"   â±ï¸ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª: {stats['avg_response_time']:.2f}s\n"
        status_msg += f"   ğŸ”„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {stats['current_concurrent']}\n"
        status_msg += f"   ğŸ”ï¸ Ø§Ù„Ø°Ø±ÙˆØ©: {stats['peak_concurrent']}\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        import psutil
        memory = psutil.virtual_memory()
        cpu = psutil.cpu_percent()
        
        status_msg += f"\nğŸ–¥ï¸ **Ø§Ù„Ù†Ø¸Ø§Ù…:**\n"
        status_msg += f"   ğŸ§  Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory.percent}% ({memory.used//1024//1024}MB)\n"
        status_msg += f"   âš™ï¸ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {cpu}%\n"
        status_msg += f"   ğŸ• ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„: {time.time() - start_time:.0f}s\n"
        
        await event.reply(status_msg)
        
    except Exception as e:
        await event.reply(f"âŒ **Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ù†Ø¸Ø§Ù…:** {e}")
        import traceback
        LOGGER(__name__).error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {traceback.format_exc()}")
